{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Kubernetes Efficient Power Level Exporter (Kepler)","text":"<p>Kepler (Kubernetes-based Efficient Power Level Exporter) is a Prometheus exporter. It uses eBPF to probe CPU performance counters and Linux kernel tracepoints.</p> <p>These data and stats from sysfs can then be fed into ML models to estimate energy consumption by Pods.</p> <p>Check out the project on GitHub \u27a1\ufe0f Kepler.</p> <p>For a comprehensive overview, please check out \u27a1\ufe0f this CNCF blog article.</p> <p></p> <p> We are a Cloud Native Computing Foundation sandbox project.  </p>"},{"location":"design/architecture/","title":"Components","text":""},{"location":"design/architecture/#kepler-exporter","title":"Kepler Exporter","text":"<p>Kepler Exporter exposes a variety of metrics about the energy consumption of Kubernetes components such as Pods and Nodes.</p> <p>Monitor container power consumption with the metrics made available by the Kepler Exporter.</p> <p></p>"},{"location":"design/architecture/#kepler-model-server","title":"Kepler Model Server","text":"<p>The main feature of <code>Kepler Model Server</code> is to return a power estimation model corresponding to the request containing target granularity (node in total, node per each processor component, pod in total, pod per each processor component), available input metrics, model filters such as accuracy.</p> <p>In addition, the online-trainer can be deployed as a sidecar container to the server (main container) to execute training pipelines and update the model on the fly when power metrics are available.</p> <p><code>Kepler Estimator</code> is a client module to Kepler model server running as a sidecar of Kepler Exporter (main container).</p> <p>This python will serve a PowerRequest from model package in Kepler Exporter as defined in estimator.go via unix domain socket <code>/tmp/estimator.sock</code>.</p> <p>Check us out on GitHub \u27a1\ufe0f Kepler Model Server</p>"},{"location":"design/ebpf_in_kepler/","title":"eBPF in Kepler","text":""},{"location":"design/ebpf_in_kepler/#background","title":"Background","text":""},{"location":"design/ebpf_in_kepler/#what-is-ebpf","title":"What is eBPF?","text":"<p>eBPF is a revolutionary technology with origins in the Linux kernel that can run sandboxed programs in a privileged context such as the operating system kernel. It is used to safely and efficiently extend the capabilities of the kernel without requiring to change kernel source code or load kernel modules. [1]</p>"},{"location":"design/ebpf_in_kepler/#what-is-a-kprobe","title":"What is a kprobe?","text":"<p>KProbes is a debugging mechanism for the Linux kernel which can also be used for monitoring events inside a production system. KProbes enables you to dynamically break into any kernel routine and collect debugging and performance information non-disruptively. You can trap at almost any kernel code address, specifying a handler routine to be invoked when the breakpoint is hit.  [2]</p>"},{"location":"design/ebpf_in_kepler/#how-to-list-all-currently-registered-kprobes","title":"How to list all currently registered kprobes?","text":"<pre><code>sudo cat /sys/kernel/debug/kprobes/list\n</code></pre>"},{"location":"design/ebpf_in_kepler/#hardware-cpu-events-monitoring","title":"Hardware CPU Events Monitoring","text":"<p>Performance counters are special hardware registers available on most modern CPUs. These registers count the number of certain types of hw events: such as instructions executed, cache misses suffered, or branches mis-predicted -without slowing down the kernel or applications. [4]</p> <p>Using syscall <code>perf_event_open</code> [5], Linux allows to set up performance monitoring for hardware and software performance. It returns a file descriptor to read performance information. This syscall takes <code>pid</code> and <code>cpuid</code> as parameters. Kepler uses <code>pid == -1</code> and <code>cpuid</code> as actual cpu id. This combination of pid and cpu allows measuring all process/threads on the specified cpu.</p>"},{"location":"design/ebpf_in_kepler/#how-to-check-if-kernel-supports-perf_event_open","title":"How to check if kernel supports <code>perf_event_open</code>?","text":"<p>Check presence of <code>/proc/sys/kernel/perf_event_paranoid</code> to know if kernel supports <code>perf_event_open</code> and what is allowed to be measured</p> <pre><code>   The perf_event_paranoid file can be set to restrict\n   access to the performance counters.\n\n   2      allow only user-space measurements (default since Linux 4.6).\n   1      allow both kernel and user measurements (default before Linux 4.6).\n   0      allow access to CPU-specific data but not raw tracepoint samples.\n  -1      no restrictions.\n\n\n   Measuring all process/threads required CAP_SYS_ADMIN capability or a value less than 1 in above file\n</code></pre> <p>CAP_SYS_ADMIN is highest level of capability, it must have some security implications</p>"},{"location":"design/ebpf_in_kepler/#kernel-routine-probed-by-kepler","title":"Kernel Routine Probed by Kepler","text":"<p>Kepler traps into <code>finish_task_switch</code> kernel function [3], which is responsible for cleaning up after a task switch occurs. Since the probe is <code>kprobe</code> it is called before <code>finish_task_switch</code> is called (instead of a <code>kretprobe</code> which is called after the probed function returns).</p> <p>When a context switch occurs inside the kernel, the function <code>finish_task_switch</code> is called on the new task which is going to use the CPU. This function receives an argument of type <code>task_struct*</code> which contains all the information about the task which is leaving the CPU.[3]</p> <p>The probe function in Kepler is</p> <pre><code>int kprobe__finish_task_switch(struct pt_regs *ctx, struct task_struct *prev)\n</code></pre> <p>The first argument is of type pointer to a <code>pt_regs</code> struct which refers to the structure that holds the register state of the CPU at the time of the kernel function entry. This struct contains fields that correspond to the CPU registers, such as general-purpose registers (e.g., r0, r1, etc.), stack pointer (sp), program counter (pc), and other architectural-specific registers. The second argument is a pointer to a <code>task_struct</code> which contains the task information for the previous task, i.e. the task which is leaving the CPU.</p>"},{"location":"design/ebpf_in_kepler/#hardware-cpu-events-monitored-by-kepler","title":"Hardware CPU events monitored by Kepler","text":"<p>Kepler opens monitoring for following hardware cpu events</p> PERF Type Perf Count Type Description Array name (in bpf program) PERF_TYPE_HARDWARE PERF_COUNT_HW_CPU_CYCLES Total CPU cycles; can get affected by CPU frequency scaling cpu_cycles_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_REF_CPU_CYCLES Total CPU cycles; not affected by CPU frequency scaling cpu_ref_cycles_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_INSTRUCTIONS Retired instructions.  Be careful, these can be affected by various issues, most notably hardware interrupt counts. cpu_instr_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_CACHE_MISSES Cache misses. Usually this indicates Last Level Cache misses; this is intended to be used in conjunction with the PERF_COUNT_HW_CACHE_REFERENCES event to calculate cache miss rates. cache_miss_hc_reader <p>Performance counters are accessed via special file descriptors. There's one file descriptor per virtual counter used. The file descriptor is associated with the corresponding array. When bcc wrapper functions are used, it reads the corresponding fd, and return values.</p>"},{"location":"design/ebpf_in_kepler/#calculate-process-aka-task-total-cpu-time","title":"Calculate process (aka task) total CPU time","text":"<p>The ebpf program (<code>bpfassets/bcc/bcc.c</code>) maintains a mapping from a <code>&lt;pid, cpuid&gt;</code> pair to a timestamp. The timestamp signifies the moment <code>kprobe__finish_task_switch</code> was called for pid when this pid was to be scheduled on cpu <code>&lt;cpuid&gt;</code></p> <pre><code>// &lt;Task PID, CPUID&gt; =&gt; Context Switch Start time\n\ntypedef struct pid_time_t { u32 pid; u32 cpu; } pid_time_t;\nBPF_HASH(pid_time, pid_time_t);\n// pid_time is the name of variable which if of type map\n</code></pre> <p>Within the function <code>get_on_cpu_time</code>, the difference between the current timestamp and timestamp from the <code>pid_time</code> map is used to calculate the <code>on_cpu_time_delta</code> for previous task on the current cpu.</p> <p>This <code>on_cpu_time_delta</code> is used to accumulate the <code>process_run_time</code> metrics for the previous task.</p>"},{"location":"design/ebpf_in_kepler/#calculate-task-cpu-cycles","title":"Calculate task CPU cycles","text":"<p>For task cpu cycles, the bpf program maintains an array named <code>cpu_cycles</code>, indexed by <code>cpuid</code>. This contains values from perf array <code>cpu_cycles_hc_reader</code>, which is a perf event type array.</p> <p>On each task switch:</p> <ul> <li>current value is read from perf counter array cpu_cycles_hc_reader</li> <li>the previous value from cpu_cycles is retrieved</li> <li>delta is calculated by subtracting prev value from current value</li> <li>the current value is copied back to cpu_cycles for next task switch</li> </ul> <p>The delta thus calculated is the cpu cycles used by the process leaving the cpu</p>"},{"location":"design/ebpf_in_kepler/#calculate-task-ref-cpu-cycles","title":"Calculate task Ref CPU cycles","text":"<p>Same process as calculating CPU cycles, difference being perf array used is <code>cpu_ref_cycles_hc_reader</code> and prev value is stored in <code>cpu_ref_cycles</code></p>"},{"location":"design/ebpf_in_kepler/#calculate-task-cpu-instructions","title":"Calculate task CPU instructions","text":"<p>Same process as calculating CPU cycles, difference being perf array used is <code>cpu_instr_hc_reader</code> and prev value is stored in <code>cpu_instr</code></p>"},{"location":"design/ebpf_in_kepler/#calculate-task-cache-misses","title":"Calculate task Cache misses","text":"<p>Same process as calculating CPU cycles, difference being perf array used is <code>cache_miss_hc_reader</code> and prev value is stored in <code>cache_miss</code></p>"},{"location":"design/ebpf_in_kepler/#calculate-on-cpu-average-frequency","title":"Calculate 'On CPU Average Frequency'","text":"<pre><code>avg_freq = ((on_cpu_cycles_delta * CPU_REF_FREQ) / on_cpu_ref_cycles_delta) * HZ;\n\nCPU_REF_FREQ = 2500\nHZ = 1000\n</code></pre> <p>This value is stored in array <code>cpu_freq_array</code></p>"},{"location":"design/ebpf_in_kepler/#calculate-page-cache-hit","title":"Calculate 'page cache hit'","text":"<p>The probe function in Kepler <code>kprobe__set_page_dirty</code> and <code>kprobe__mark_page_accessed</code> are used to track page cache hit for write and read action respectively.</p>"},{"location":"design/ebpf_in_kepler/#process-table","title":"Process Table","text":"<p>The bpf program maintains a bpf hash named <code>processes</code>. This hash maintains data calculated for a process. Kepler reads values from this hash ( known as a <code>Table</code> in bcc ) and generates metrics.</p> Key Value Description pid cgroupid Process CGroupID pid Process ID process_run_time Total time a process occupies CPU (calculated each time process leaves CPU on context switch) cpu_cycles Total CPU cycles consumed by process cpu_instr Total CPU instructions consumed by process cache_miss Total Cache miss by process page_cache_hit Total hit of the page cache vec_nr Total number of soft irq handles by process (max 10) comm Process name (max length 16) <p>This hash is read by the kernel collector in <code>container_hc_collector.go</code> for metrics collection.</p>"},{"location":"design/ebpf_in_kepler/#references","title":"References","text":"<p>[1] https://ebpf.io/what-is-ebpf/ , https://www.splunk.com/en_us/blog/learn/what-is-ebpf.html , https://www.tigera.io/learn/guides/ebpf/</p> <p>[2] An introduction to KProbes , Kernel Probes (Kprobes)</p> <p>[3] finish_task_switch - clean up after a task-switch</p> <p>[4] Performance Counters for Linux</p> <p>[5] perf_event_open(2) \u2014 Linux manual page</p>"},{"location":"design/kepler-energy-sources/","title":"Kepler Energy Sources","text":""},{"location":"design/kepler-energy-sources/#background","title":"Background","text":""},{"location":"design/kepler-energy-sources/#rapl-running-average-power-limit","title":"RAPL - Running Average Power Limit","text":"<p>Intel's Running Average Power Limit (RAPL) is a hardware feature which allows to monitor energy consumption across different domains of the CPU chip, attached DRAM and on-chip GPU. This feature was introduced in Intel's Sandy Bridge architecture and has evolved in the later versions of Intel's processing architecture. With RAPL it is possible to programmatically get real time data on the power consumption of the CPU package and its components, as well as of the DRAM memory that the CPU is managing.</p> <p>RAPL provides two different functionalities:</p> <ol> <li>Allows energy consumption to be measured at very fine granularity and a high sampling    rate.</li> <li>Allows limiting (or capping) the average power consumption of different components inside    the processor, which also limits the thermal output of the processor</li> </ol> <p>Kepler makes use of the energy consumption measurement capability.</p> <p>RAPL supports multiple power domains. The RAPL power domain is a physically meaningful domain (e.g., Processor Package, DRAM etc) for power management. Each power domain informs the energy consumption of the domain.</p> <p>RAPL provides the following power domains for both measuring and limiting energy consumption:</p> <ul> <li>Package: Package (PKG) domain measures the energy consumption of the entire socket. It   includes the consumption of all the cores, integrated graphics and also the uncore components   (last level caches, memory controller).</li> <li>Power Plane 0 (PP0) : measures the energy consumption of all processor cores on the socket.</li> <li>Power Plane 1 (PP1) : measures the energy consumption of processor graphics (GPU) on the   socket (desktop models only).</li> <li>DRAM: measures the energy consumption of random access memory (RAM) attached to the integrated   memory controller.</li> </ul> <p>The support for different power domains varies according to the processor model.</p>"},{"location":"design/kepler-energy-sources/#reading-energy-values","title":"Reading Energy values","text":"<p>Kepler chooses to use one energy source in the following order of preference:</p> <ol> <li>Sysfs</li> <li>MSR</li> <li>Hwmon</li> </ol>"},{"location":"design/kepler-energy-sources/#using-rapl-sysfs","title":"Using RAPL Sysfs","text":"<p>From Linux Kernel version 3.13 onwards, RAPL values can be read using <code>Power Capping Framework</code>[2].</p> <p>Linux Power Capping framework exposes power capping devices to user space via sysfs in the form of a tree of objects.</p> <p>This sysfs tree is mounted at <code>/sys/class/powercap/intel-rapl</code>. When RAPL is available, this path exists and Kepler reads energy values from this path.</p>"},{"location":"design/kepler-energy-sources/#using-rapl-msr-model-specific-registers","title":"Using RAPL MSR (Model Specific Registers)","text":"<p>The RAPL energy counters can be accessed through model-specific registers (MSRs). The counters are 32-bit registers that indicate the energy consumed since the processor was booted up. The counters are updated approximately once every millisecond. The energy is counted in multiples of model-specific energy units. Sandy Bridge uses energy units of 15.3 microjoules, whereas Haswell and Skylake uses units of 61 microjoules. The units can be read from specific MSRs before doing energy calculations.</p> <p>The MSRs can be accessed directly on Linux using the msr driver in the kernel. Reading RAPL domain values directly from MSRs requires detecting the CPU model and reading the RAPL energy units before reading the RAPL domain. Once the CPU model is detected, the RAPL domains can be read per package of the CPU by reading the corresponding <code>MSR status</code> register.</p> <p>There are basically two types of events that RAPL events report Static Events: thermal specifications, maximum and minimum power caps, and time windows. Dynamic Events: RAPL domain energy readings from the chip such as PKG, PP0, PP1 or DRAM</p>"},{"location":"design/kepler-energy-sources/#using-kernel-driver-xgene-hwmon","title":"Using kernel driver xgene-hwmon","text":"<p>Using Xgene-hwmon driver Kepler reads power from APM X-Gene SoC. It supports reading CPU and IO power in micro watts.</p>"},{"location":"design/kepler-energy-sources/#using-ebpf-perf-events","title":"Using eBpf perf events","text":"<p>Not used in Kepler.</p>"},{"location":"design/kepler-energy-sources/#using-papi-library","title":"Using PAPI library","text":"<p>Performance Application Programming Interface (PAPI) is not used in Kepler.</p>"},{"location":"design/kepler-energy-sources/#permissions-required","title":"Permissions required","text":""},{"location":"design/kepler-energy-sources/#sysfs-powercap","title":"Sysfs (powercap)","text":"<p>Root access is required to use powercap driver.</p>"},{"location":"design/kepler-energy-sources/#msrs","title":"MSRs","text":"<p>Root access is required to use the msr driver.</p>"},{"location":"design/kepler-energy-sources/#references","title":"References","text":"<p>[1] RAPL in Action: Experiences in Using RAPL for Power Measurements</p> <p>[2] RA Power Capping Framework</p> <p>[3] RA Kernel driver xgene-hwmon</p> <p>[4] RA Performance Application Programming Interface (PAPI)</p>"},{"location":"design/metrics/","title":"Monitoring Container Power Consumption with Kepler","text":"<p>Kepler Exporter exposes statistics from an application running in a Kubernetes cluster in a Prometheus-friendly format that can be scraped by any database that understands this format, such as Prometheus and Sysdig.</p> <p>Kepler exports a variety of container metrics to Prometheus, where the main ones are those related to energy consumption.</p>"},{"location":"design/metrics/#kepler-metrics-overview","title":"Kepler Metrics Overview","text":"<p>All the metrics specific to the Kepler Exporter are prefixed with <code>kepler</code>.</p>"},{"location":"design/metrics/#kepler-metrics-for-container-energy-consumption","title":"Kepler Metrics for Container Energy Consumption","text":"<ul> <li> <p>kepler_container_joules_total (Counter)</p> <p>This metric is the aggregated package/socket energy consumption of CPU, dram, gpus, and other host components for a given container. Each component has individual metrics which are detailed next in this document.</p> <p>This metric simplifies the Prometheus metric for performance reasons. A very large promQL query typically introduces a very high overhead on Prometheus.</p> </li> <li> <p>kepler_container_core_joules_total (Counter)</p> <p>This measures the total energy consumption on CPU cores that  a certain container has used. Generally, when the system has access to RAPL metrics, this metric will reflect the proportional container energy consumption of the RAPL Power Plan 0 (PP0), which is the energy consumed by all CPU cores in the socket. However, this metric is processor model specific and may not be available on some server CPUs. The RAPL CPU metric that is available on all processors that support RAPL is the package, which we will detail on another metric.</p> <p>In some cases where RAPL is available but core metrics are not, Kepler may use the energy consumption package. But note that package energy consumption is not just from CPU cores, it is all socket energy consumption.</p> <p>In case RAPL is not available, Kepler might estimate this metric using the model server.</p> </li> <li> <p>kepler_container_dram_joules_total (Counter)</p> <p>This metric describes the total energy spent in DRAM by a container.</p> </li> <li> <p>kepler_container_uncore_joules_total (Counter)</p> <p>This measures the cumulative energy consumed by certain uncore components, which are typically the last level cache, integrated GPU and memory controller, but the number of components may vary depending on the system. The uncore metric is processor model specific and may not be available on some server CPUs.</p> <p>When RAPL is not available, Kepler can estimate this metric using the model server if the node CPU supports the uncore metric.</p> </li> <li> <p>kepler_container_package_joules_total (Counter)</p> <p>This measures the cumulative energy consumed by the CPU socket, including all cores and uncore components (e.g. last-level cache, integrated GPU and memory controller). RAPL package energy is typically the PP0 + PP1, but PP1 counter may or may not account for all energy usage by uncore components. Therefore, package energy consumption may be higher than core + uncore.</p> <p>When RAPL is not available, Kepler might estimate this metric using the model server.</p> </li> <li> <p>kepler_container_other_joules_total (Counter)</p> <p>This measures the cumulative energy consumption on other host components besides the CPU and DRAM. The vast majority of motherboards have a energy consumption sensor that can be accessed via the kernel acpi or ipmi. This sensor reports the energy consumption of the entire system. In addition, some processor architectures support the RAPL platform domain (PSys) which is the energy consumed by the \"System on a chipt\" (SOC).</p> <p>Generally, this metric is the host energy consumption (from acpi) less the RAPL Package and DRAM.</p> </li> <li> <p>kepler_container_gpu_joules_total (Counter)</p> <p>This measures the total energy consumption on the GPUs that  a certain container has used. Currently, Kepler only supports NVIDIA GPUs, but this metric will also reflect other accelerators in the future. So when the system has NVIDIA GPUs, Kepler can calculate the energy consumption of the container's gpu using the GPU's processes energy consumption and utilization via NVIDIA nvml package.</p> </li> <li> <p>kepler_container_energy_stat (Counter)</p> <p>This metric contains several container metrics labeled with container resource utilization cgroup metrics that are used in the model server for predictions.</p> <p>This metric is specific for the model server and might be updated any time.</p> </li> </ul>"},{"location":"design/metrics/#kepler-metrics-for-container-resource-utilization","title":"Kepler Metrics for Container Resource Utilization","text":""},{"location":"design/metrics/#base-metric","title":"Base Metric","text":"<ul> <li> <p>kepler_container_bpf_cpu_time_us_total</p> <p>This measures the total CPU time used by the container using BPF tracing. This is a minimum exposed metric.</p> </li> </ul>"},{"location":"design/metrics/#hardware-counter-metrics","title":"Hardware Counter Metrics","text":"<ul> <li> <p>kepler_container_cpu_cycles_total</p> <p>This measures the total CPU cycles used by the container using hardware counters. To support fine-grained analysis of performance and resource utilization, hardware counters are particularly desirable due to its granularity and precision..</p> <p>The CPU cycles is a metric directly related to CPU frequency. On systems where processors run at a fixed frequency, CPU cycles and total CPU time are roughly equivalent. On systems where processors run at varying frequencies, CPU cycles and total CPU time will have different values.</p> </li> <li> <p>kepler_container_cpu_instructions_total</p> <p>This measure the total cpu instructions used by the container using hardware counters.</p> <p>CPU instructions are the de facto metric for accounting for CPU utilization.</p> </li> <li> <p>kepler_container_cache_miss_total</p> <p>This measures the total cache miss that has occurred for a given container using hardware counters.</p> <p>As there is no event counter that measures memory access directly, the number of last-level cache misses gives a good proxy for the memory access number. If an LLC read miss occurs, a read access to main memory should occur (but note that this is not necessarily the case for LLC write misses under a write-back cache policy).</p> </li> </ul> <p>Note</p> <p>You can enable/disable expose of those metrics through <code>expose-hardware-counter-metrics</code> Kepler execution option or <code>EXPOSE_HW_COUNTER_METRICS</code> environment value.</p>"},{"location":"design/metrics/#irq-metrics","title":"IRQ Metrics","text":"<ul> <li> <p>kepler_container_bpf_net_tx_irq_total</p> <p>This measures the total transmitted packets to network cards of the container using BPF tracing.</p> </li> <li> <p>kepler_container_bpf_net_rx_irq_total</p> <p>This measures the total packets received from network cards of the container using BPF tracing.</p> </li> <li> <p>kepler_container_bpf_block_irq_total</p> <p>This measures block I/O called of the container using BPF tracing.</p> </li> </ul> <p>Note</p> <p>You can enable/disable expose of those metrics through <code>EXPOSE_IRQ_COUNTER_METRICS</code> environment value.</p>"},{"location":"design/metrics/#kepler-metrics-for-node-information","title":"Kepler Metrics for Node Information","text":"<ul> <li> <p>kepler_node_info (Counter)</p> <p>This metric shows the node metadata like the node CPU architecture.</p> </li> </ul>"},{"location":"design/metrics/#kepler-metrics-for-node-energy-consumption","title":"Kepler Metrics for Node Energy Consumption","text":"<ul> <li> <p>kepler_node_core_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_uncore_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_dram_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_package_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_other_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_gpu_joules_total (Counter)</p> <p>Similar to container metrics, but representing the aggregation of all containers running on the node and operating system (i.e. \"system_process\").</p> </li> <li> <p>kepler_node_platform_joules_total (Counter)</p> <p>This metric represents the total energy consumption of the host.</p> <p>The vast majority of motherboards have a energy consumption sensor that can be accessed via the acpi or ipmi kernel. This sensor reports the energy consumption of the entire system. In addition, some processor architectures support the RAPL platform domain (PSys) which is the energy consumed by the \"System on a chipt\" (SOC).</p> <p>Generally, this metric is the host energy consumption from Redfish BMC or acpi.</p> </li> <li> <p>kepler_node_energy_stat (Counter)</p> <p>This metric contains multiple metrics from nodes labeled with container resource utilization cgroup metrics that are used in the model server.</p> <p>This metric is specific to the model server and can be updated at any time.</p> </li> </ul> <p>Note</p> <p>\"system_process\" is a special indicator that aggregate all the non-container workload into system process consumption metric.</p>"},{"location":"design/metrics/#kepler-metrics-for-node-resource-utilization","title":"Kepler Metrics for Node Resource Utilization","text":""},{"location":"design/metrics/#accelerator-metrics","title":"Accelerator Metrics","text":"<ul> <li> <p>kepler_node_accelerator_intel_qat</p> <p>This measures the utilization of the accelerator Intel QAT on a certain node. When the system has Intel QATs, Kepler can calculate the utilization of the node's QATs through telemetry.</p> </li> </ul>"},{"location":"design/metrics/#exploring-node-exporter-metrics-through-the-prometheus-expression","title":"Exploring Node Exporter Metrics Through the Prometheus Expression","text":"<p>All the energy consumption metrics are defined as counter following the Prometheus metrics guide for energy related metrics.</p> <p>The <code>rate()</code> of joules gives the power in Watts since the rate function returns the average per second. Therefore, for get the container energy consumption you can use the following query:</p> <pre><code>sum by (pod_name, container_name, container_namespace, node)(irate(kepler_container_joules_total{}[1m]))\n</code></pre> <p>Note that we report the node label in the container metrics because the OS metrics \"system_process\" will have the same name and namespace across all nodes and we do not want to aggregate them.</p>"},{"location":"design/metrics/#rapl-power-domain","title":"RAPL Power Domain","text":"<p>RAPL power domains supported in some resent Intel microarchitecture (consumer-grade/server-grade):</p> Microarchitecture Package CORE (PP0) UNCORE (PP1) DRAM Haswell Y/Y Y/N Y/N Y/Y Broadwell Y/Y Y/N Y/N Y/Y Skylake Y/Y Y/Y Y/N Y/Y Kaby Lake Y/Y Y/Y Y/N Y/Y"},{"location":"design/power_model/","title":"Kepler Power Model","text":"<p>In Kepler, with respective to available measurements, we provide a pod-level power with a mix of two power modeling approaches:</p>"},{"location":"design/power_model/#modeling-approach","title":"Modeling Approach","text":"<ul> <li> <p>Power Ratio Modeling: This modeling computes a finer-grained power by the usage ratio over the   total summation of power. This modeling is used by default when the total power is known.</p> </li> <li> <p>Power Estimation Modeling: This modeling estimates a power by using usage metrics as input features of the trained model. This modeling can be used even if the power metric cannot be measured. The estimation can be done in three levels: Node total power (including fan, power supply, etc.), Node internal component powers (such as CPU, Memory), Pod power.</p> <p>Note</p> <p>Also see Get started with Kepler Model Server</p> </li> <li> <p>Pre-trained Power Models: We provide pre-trained power models for different deployment scenarios.    Current x86_64 pre-trained model are developed in Intel\u00ae Xeon\u00ae Processor E5-2667 v3. Models with    other architectures are coming soon. You can find these models in Kepler Model DB. These models    support both power ratio modeling and power estimation modeling for both RAPL and ACPI power sources.    The <code>AbsPower</code> models estimate both idle and dynamic power while the <code>DynPower</code> models only estimate    dynamic power. The MAE (Mean Absolute Error) of these models are also published. Kepler container    image has preloaded acpi/AbsPower/BPFOnly/SGDRegressorTrainer_1.json model for node energy estimate    and rapl/AbsPower/BPFOnly/SGDRegressorTrainer_1.json for Container absolute power estimate.</p> </li> </ul>"},{"location":"design/power_model/#usage-scenario","title":"Usage Scenario","text":"Scenario Node Total Power Node Component Powers Pod Power BM (x86 with power meter) Measurement (e.g., ACPI) Measurement (RAPL) Power Ratio BM (x86 but no power meter) Power Estimation Measurement Power Ratio BM (non-x86 with power meter) Measurement Power Estimation Power Ratio BM (non-x86 and no power meter) Power Estimation Power Estimation Power Ratio VM with node info and power passthrough from BM (x86 with power meter) Measurement + VM Mapping Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (x86 but no power meter) Power Estimation Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (non-x86 with power meter) Measurement + VM Mapping Power Estimation Power Ratio VM with node info Power Estimation Power Estimation Power Ratio Pure VM - - Power Estimation"},{"location":"hardwareengagement/","title":"Hardware Engagement","text":"<p>In this document, we will share our steps as how to engage Kepler with a specific hardware device. Considering there are many different hardware devices, we will just write down major steps here as current stage. You are able to take this out as a todo list and step by step to make Kepler engage with your own hardware device.</p>"},{"location":"hardwareengagement/#stage-0-proof","title":"Stage 0 Proof","text":"<p>In this Stage, we will focus on basic data can be collected by golang, and you can build your own Kepler which running on your device well. The following steps can running in parallel.</p>"},{"location":"hardwareengagement/#binary-build-and-container-build","title":"Binary build and container build","text":"<p>Currently Kepler container image is from a GPU image to support GPU case. Considering a general case for IOT device. You may need to build Kepler from UBI image. We recommend you following steps below to setup a local build env and try to build.</p> <ol> <li>Find a linux OS.</li> <li>Install Kepler dependencies as eBPF golang(BCC), linux header and build Kepler (from main branch or latest release branch) binary.</li> <li>(Optional)Modify dockerfile to build the container image.</li> </ol>"},{"location":"hardwareengagement/#power-consumption-api","title":"Power consumption API","text":"<p>Currently, we use power consumption API as RAPL or ACPI. For some of the devices, you may need to find your own way to get power consumption, and implement in golang for Kepler usage. For further plan, please ref here</p>"},{"location":"hardwareengagement/#ebpf-data","title":"eBPF data","text":"<p>Currently, we rely on eBPF to obtain key cpu, irq  and perf information about a process. Hence,  refer to the documentation of cilium/ebpf to test whether these Go packages work well on your device.</p> <p>Please let us know if you need any further adjustments!</p>"},{"location":"hardwareengagement/#stage-1-integration-with-ratio","title":"Stage 1 Integration with ratio","text":"<p>During this Stage, we are going to ref Kepler model. To integrate and implement your own logic specific to your device and deep dive into Power consumption API.</p>"},{"location":"hardwareengagement/#scope","title":"Scope","text":"<p>You should know the scope of the Power consumption API. How many API do you have? Is it categorized by CPU/memory/IO or not?</p>"},{"location":"hardwareengagement/#interval","title":"Interval","text":"<p>You should know the intervals of the Power consumption API. As Kepler collects eBPF data in each 3s by default, you should know the interval and make them in same time slot.</p>"},{"location":"hardwareengagement/#verify","title":"Verify","text":"<p>You can cross check and verify the data.</p>"},{"location":"hardwareengagement/#stage-2-model_training","title":"Stage 2 Model_training","text":"<p>TBD</p>"},{"location":"installation/Kepler-Telegraf-integration-steps/","title":"Introduction","text":"<p>Kepler (Kubernetes-based Efficient Power Level Exporter) is a Prometheus exporter. It uses eBPF to probe CPU performance counters and Linux kernel tracepoints [1] whereas Telegraf is an agent for collecting, processing, aggregating, and writing metrics. [2]This document covers the steps for integrating Telegraf with Kepler.</p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#benefits-of-integrating-telegraf-with-kepler","title":"Benefits of Integrating Telegraf with Kepler","text":"<p>Integrating Telegraf with Kepler helps users to gather additional platform level metrics on top of Kepler metrics. Kepler provide useful container and Node metrics. On the other hand, through Telegraf, metrics like Power Supply Current output (%) can be gathered using IPMI Sensor plugin. Also, it can help to gather DPDK related metrics which is currently not possible through Kepler. By correlating power and CPU usage metrics from Kepler and DPDK metrics from Telegraf, user will gain a better understanding about the power usage of their packet processing application and can use these insights as inputs to identify opportunities for power optimization. Hence, Kepler and Telegraf metrics together can serve use cases that help end users to understand and optimize power usage by their various networking applications.</p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#setup","title":"Setup","text":""},{"location":"installation/Kepler-Telegraf-integration-steps/#setup-details","title":"Setup Details","text":"<p>The Control plane server details are as follows:</p> Components Details Model Intel(R) Xeon(R) Gold 6230N CPU @ 2.30GHz Sockets 2 Cores per sockets 20 Total Cores 80 Software Ubuntu 22.04.1 LTS"},{"location":"installation/Kepler-Telegraf-integration-steps/#download-and-install-kepler","title":"Download and Install Kepler","text":"<p>There are various ways Kepler can be downloaded and installed. For more details on each steps please refer to the Kepler documents.</p> <pre><code>root@: git clone https://github.com/sustainable-computing-io/kepler.git\nroot@: cd kepler/\nroot@: make build-manifest OPTS=\"BM_DEPLOY PROMETHEUS_DEPLOY\"\nroot@: cd _output/generated-manifest/\nroot@: vi deployment.yaml\nroot@: kubectl apply -f _output/generated-manifest/deployment.yaml\n</code></pre> <p>Installation of Kepler can be confirmed through following commands:</p> <pre><code>root@: docker ps -a | grep 'kepler'\n\n530a71f0067f        quay.io/sustainable_computing_io/kepler           \"/bin/sh \u2013\nc '/usr/bi\u2026\"   33 seconds ago      Up 31 seconds\nk8s_kepler-exporter_kepler-exporter-bzj9b_kepler_827ee818-9f5a-460c-a368-\nfc90fde5d378_0\ndecae0dc60e2        k8s.gcr.io/pause:3.3                              \"/pause\"\n38 seconds ago      Up 35 seconds\nk8s_POD_kepler-exporter-bzj9b_kepler_827ee818-9f5a-460c-a368-fc90fde5d378_0\n\nroot@:~# kubectl get pod -n kepler\nNAME                    READY   STATUS    RESTARTS   AGE\nkepler-exporter-8h8x7   1/1     Running   0          63s\nkepler-exporter-bzj9b   1/1     Running   0          63s\nroot@:~# kubectl port-forward kepler-exporter-jdklk 9102:9102 -n kepler --address='0.0.0.0'\n</code></pre>"},{"location":"installation/Kepler-Telegraf-integration-steps/#download-and-start-telegraf","title":"Download and start Telegraf","text":"<p>Telegraf can be installed on the system in various ways. Here it has been done by downloading and building it from source.</p> <p>Telegraf requires Go version &gt;=1.22 which can be installed : Install Go and the Makefile requires GNU make.</p> <p>Telegraf shares the same minimum requirements as Go:</p> <ul> <li>Linux kernel version 2.6.32 or later</li> <li>Windows 10 or later</li> <li>FreeBSD 12 or later</li> <li>macOS 10.15 Catalina or later</li> </ul> <p>Clone the Telegraf repository:</p> <pre><code>root@:~# git clone https://github.com/influxdata/telegraf.git\n</code></pre> <p>Run make build from the source directory</p> <pre><code>root@:~# cd telegraf\nroot@:~# make build\n</code></pre> <p>Generate a Telegraf config file</p> <pre><code>root@:~# telegraf config &gt; telegraf.conf\n</code></pre> <p>Edit the generated config file to enable required plugins. For this integration activity following plugins should be enabled:</p> <p>Input Plugins: Intel PowerStat plugin, Intel PMU plugin, and IPMI sensor input</p> <p>Output Plugin: Prometheus output plugin must be enabled in Telegraf config in order to store the metrics in Prometheus database .</p> <p>Below is the sample config that have been used to enable all the above-mentioned plugins. Although, user can enable any other desired plugin by commenting out the respective section.</p> <pre><code>root@:~# vi telegraf.conf\n\n# Global tags can be specified here in key=\"value\" format.\n[global_tags]\n  # dc = \"us-east-1\" # will tag all metrics with dc=us-east-1\n  # rack = \"1a\"\n  ## Environment variables can be used as tags, and throughout the config file\n  # user = \"$USER\"\n\n# Configuration for telegraf agent\n[agent]\n  ## Default data collection interval for all inputs\n  interval = \"10s\"\n  ## Rounds collection interval to 'interval'\n  ## ie, if interval=\"10s\" then always collect on :00, :10, :20, etc.\n  round_interval = true\n\n  ## Telegraf will send metrics to outputs in batches of at most\n  ## metric_batch_size metrics.\n  ## This controls the size of writes that Telegraf sends to output plugins.\n  metric_batch_size = 1000\n\n  ## Maximum number of unwritten metrics per output.  Increasing this value\n  ## allows for longer periods of output downtime without dropping metrics at the\n  ## cost of higher maximum memory usage.\n  metric_buffer_limit = 10000\n\n  ## Collection jitter is used to jitter the collection by a random amount.\n  ## Each plugin will sleep for a random time within jitter before collecting.\n  ## This can be used to avoid many plugins querying things like sysfs at the\n  ## same time, which can have a measurable effect on the system.\n  collection_jitter = \"0s\"\n\n  ## Default flushing interval for all outputs. Maximum flush_interval will be\n  ## flush_interval + flush_jitter\n  flush_interval = \"10s\"\n  ## Jitter the flush interval by a random amount. This is primarily to avoid\n  ## large write spikes for users running a large number of telegraf instances.\n  ## ie, a jitter of 5s and interval 10s means flushes will happen every 10-15s\n  flush_jitter = \"0s\"\n\n  ## By default or when set to \"0s\", precision will be set to the same\n  ## timestamp order as the collection interval, with the maximum being 1s.\n  ##   ie, when interval = \"10s\", precision will be \"1s\"\n  ##       when interval = \"250ms\", precision will be \"1ms\"\n  ## Precision will NOT be used for service inputs. It is up to each individual\n  ## service input to set the timestamp at the appropriate precision.\n  ## Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\".\n  precision = \"\"\n\n  ## Log at debug level.\n  # debug = false\n  ## Log only error level messages.\n  # quiet = false\n\n  ## Log target controls the destination for logs and can be one of \"file\",\n  ## \"stderr\" or, on Windows, \"eventlog\".  When set to \"file\", the output file\n  ## is determined by the \"logfile\" setting.\n  # logtarget = \"file\"\n\n  ## Name of the file to be logged to when using the \"file\" logtarget.  If set to\n  ## the empty string then logs are written to stderr.\n  # logfile = \"\"\n\n  ## The logfile will be rotated after the time interval specified.  When set\n  ## to 0 no time based rotation is performed.  Logs are rotated only when\n  ## written to, if there is no log activity rotation may be delayed.\n  # logfile_rotation_interval = \"0d\"\n\n  ## The logfile will be rotated when it becomes larger than the specified\n  ## size.  When set to 0 no size based rotation is performed.\n  # logfile_rotation_max_size = \"0MB\"\n\n  ## Maximum number of rotated archives to keep, any older logs are deleted.\n  ## If set to -1, no archives are removed.\n  # logfile_rotation_max_archives = 5\n\n  ## Pick a timezone to use when logging or type 'local' for local time.\n  ## Example: America/Chicago\n  # log_with_timezone = \"\"\n\n  ## Override default hostname, if empty use os.Hostname()\n  hostname = \"\"\n  ## If set to true, do no set the \"host\" tag in the telegraf agent.\n  omit_hostname = false\n\n###############################################################################\n#                            OUTPUT PLUGINS                                   #\n###############################################################################\n\n# # Configuration for the Prometheus client to spawn\n[[outputs.prometheus_client]]\n  ## Address to listen on\n  listen = \":9273\"\n\n  ## Metric version controls the mapping from Telegraf metrics into\n  ## Prometheus format.  When using the prometheus input, use the same value in\n  ## both plugins to ensure metrics are round-tripped without modification.\n  ##\n  ##   example: metric_version = 1;\n  ##            metric_version = 2; recommended version\n  # metric_version = 1\n\n\n###############################################################################\n#                            INPUT PLUGINS                                    #\n###############################################################################\n\n# # Intel PowerStat plugin enables monitoring of platform metrics (power, TDP) and Core metrics like temperature, power and utilization.\n[[inputs.intel_powerstat]]\n  ## All global metrics are always collected by Intel PowerStat plugin.\n  ## User can choose which per-CPU metrics are monitored by the plugin in cpu_metrics array.\n  ## Empty array means no per-CPU specific metrics will be collected by the plugin - in this case only platform level\n  ## telemetry will be exposed by Intel PowerStat plugin.\n  #the package_metrics setting:\n  package_metrics = [\"current_power_consumption\", \"current_dram_power_consumption\", \"thermal_design_power\", \"max_turbo_frequency\", \"uncore_frequency\", \"cpu_base_frequency\"]\n  ## Supported options:\n  ## \"cpu_frequency\", \"cpu_busy_frequency\", \"cpu_temperature\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\"\n  cpu_metrics = [\"cpu_frequency\", \"cpu_busy_frequency\", \"cpu_temperature\", \"cpu_c1_state_residency\", \"cpu_c6_state_residency\", \"cpu_busy_cycles\"]\n\n# Intel Performance Monitoring Unit plugin exposes Intel PMU metrics available through Linux Perf subsystem\n# This plugin ONLY supports Linux on amd64\n[[inputs.intel_pmu]]\n  ## List of filesystem locations of JSON files that contain PMU event definitions.\n  event_definitions = [\"/root/.cache/pmu-events/GenuineIntel-6-55-7-core.json\", \"/root/.cache/pmu-events/GenuineIntel-6-55-7-uncore.json\"]\n\n  ## List of core events measurement entities. There can be more than one core_events sections.\n  [[inputs.intel_pmu.core_events]]\n    ## List of events to be counted. Event names shall match names from event_definitions files.\n    ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n    ## If absent, all core events from provided event_definitions are counted skipping unresolvable ones.\n    events = [\"INST_RETIRED.ANY\", \"CPU_CLK_UNHALTED.THREAD_ANY:config1=0x4043200000000k\"]\n\n    ## Limits the counting of events to core numbers specified.\n    ## If absent, events are counted on all cores.\n    ## Single \"0\", multiple \"0,1,2\" and range \"0-2\" notation is supported for each array element.\n    ##   example: cores = [\"0,2\", \"4\", \"12-16\"]\n    cores = [\"0\"]\n\n    ## Indicator that plugin shall attempt to run core_events.events as a single perf group.\n    ## If absent or set to false, each event is counted individually. Defaults to false.\n    ## This limits the number of events that can be measured to a maximum of available hardware counters per core.\n    ## Could vary depending on type of event, use of fixed counters.\n    # perf_group = false\n\n    ## Optionally set a custom tag value that will be added to every measurement within this events group.\n    ## Can be applied to any group of events, unrelated to perf_group setting.\n    # events_tag = \"\"\n\n  ## List of uncore event measurement entities. There can be more than one uncore_events sections.\n  [[inputs.intel_pmu.uncore_events]]\n    ## List of events to be counted. Event names shall match names from event_definitions files.\n    ## Single entry can contain name of the event (case insensitive) augmented with config options and perf modifiers.\n    ## If absent, all uncore events from provided event_definitions are counted skipping unresolvable ones.\n    events = [\"UNC_CHA_CLOCKTICKS\", \"UNC_CHA_TOR_OCCUPANCY.IA_MISS\"]\n\n    ## Limits the counting of events to specified sockets.\n    ## If absent, events are counted on all sockets.\n    ## Single \"0\", multiple \"0,1\" and range \"0-1\" notation is supported for each array element.\n    ##   example: sockets = [\"0-2\"]\n    sockets = [\"0\"]\n\n    ## Indicator that plugin shall provide an aggregated value for multiple units of same type distributed in an uncore.\n    ## If absent or set to false, events for each unit are exposed as separate metric. Defaults to false.\n    # aggregate_uncore_units = false\n\n    ## Optionally set a custom tag value that will be added to every measurement within this events group.\n    # events_tag = \"\"\n# Read metrics from the bare metal servers via IPMI\n[[inputs.ipmi_sensor]]\n  ## optionally specify the path to the ipmitool executable\n  # path = \"/usr/bin/ipmitool\"\n  ##\n  ## Setting 'use_sudo' to true will make use of sudo to run ipmitool.\n  ## Sudo must be configured to allow the telegraf user to run ipmitool\n  ## without a password.\n  use_sudo = true\n  ##\n  ## optionally force session privilege level. Can be CALLBACK, USER, OPERATOR, ADMINISTRATOR\n  # privilege = \"ADMINISTRATOR\"\n  ##\n  ## optionally specify one or more servers via a url matching\n  ##  [username[:password]@][protocol[(address)]]\n  ##  e.g.\n  ##    root:passwd@lan(127.0.0.1)\n  ##\n  ## if no servers are specified, local machine sensor stats will be queried\n  ##\n  # servers = [\"USERID:PASSW0RD@lan(192.168.1.1)\"]\n\n  ## Recommended: use metric 'interval' that is a multiple of 'timeout' to avoid\n  ## gaps or overlap in pulled data\n  interval = \"30s\"\n\n  ## Timeout for the ipmitool command to complete. Default is 20 seconds.\n  timeout = \"20s\"\n\n  ## Schema Version: (Optional, defaults to version 1)\n  metric_version = 2\n\n  ## Optionally provide the hex key for the IMPI connection.\n  # hex_key = \"\"\n\n  ## If ipmitool should use a cache\n  ## for me ipmitool runs about 2 to 10 times faster with cache enabled on HP G10 servers (when using ubuntu20.04)\n  ## the cache file may not work well for you if some sensors come up late\n  # use_cache = false\n\n  ## Path to the ipmitools cache file (defaults to OS temp dir)\n  ## The provided path must exist and must be writable\n  # cache_path = \"\"\n</code></pre> <p>Run Telegraf with the plugins defined in config file:</p> <pre><code>root@:~#./telegraf --config telegraf.conf\n</code></pre>"},{"location":"installation/Kepler-Telegraf-integration-steps/#download-and-start-the-prometheus-container","title":"Download and start the Prometheus container","text":"<p>Prometheus can be installed on a system in various ways. Here it is downloaded and installed as a container.</p> <p>Create a Prometheus configuration file that is scrapping from both Kepler and Telegraf instance:</p> <p>Sample Prometheus configuration file is as follows:</p> <pre><code># my global config\nglobal:\nscrape_interval:     15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.\nevaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.\n# scrape_timeout is set to the global default (10s).\n\n# A scrape configuration containing exactly one endpoint to scrape:\n# Here it's Prometheus itself.\nscrape_configs:\n# The job name is added as a label `job=&lt;job_name&gt;` to any timeseries scraped from this config.\n    - job_name: 'kepler'\n      static_configs:\n        - targets: ['xx.xx.xx:9102']\n    - job_name: 'telegraf'\n      static_configs:\n        - targets: ['xx.xx.xx:9273']\n</code></pre> <p>Run the Prometheus container with the created Prometheus configuration file:</p> <pre><code>root@:~# docker run -d -p 9090:9090 -v $PWD/prometheus.yaml:/etc/prometheus/prometheus.yml prom/prometheus\n</code></pre> <p>On the Prometheus GUI at localhost:9090, it can be confirmed that Prometheus is scrapping from Kepler and Telegraf.</p> <p></p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#download-and-start-the-grafana-container","title":"Download and start the Grafana container","text":"<p>Like, Prometheus, Grafana can be installed on the system in various ways. Here, we are installing Grafana's container image.</p> <pre><code>root@:~# docker run -d --network host --name grafana grafana/grafana\n</code></pre> <p>Once Grafana container is running access the Grafana GUI at localhost:3000. Login with default credentials. After login, The Prometheus database needs to be added as a data source into Grafana GUI. Click on <code>DATA SOURCES</code> -&gt; <code>Add your first data source</code> and select Prometheus - &gt; Click <code>Save and Test</code></p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#dashboard","title":"Dashboard","text":"<p>Once Prometheus has been added as a data source, create a dashboard by exporting Kepler-Exporter.json, a default Grafana dashboard for Kepler.</p> <p>This default Kepler dashboard can be edited in order to display Telegraf metric along with Kepler metrics.</p> <p>For example in below shown example, right hand shows Power related metrics collected by Telegraf whereas left hand shows Power related metrics by Kepler per namespace:</p> <p></p> <p>On Kepler side:</p> <p>PKG-&gt; Represents <code>kepler_container_package_joules_total</code> metrics which measures the cumulative energy consumed by the CPU socket, including all cores and uncore components (e.g. last-level cache, integrated GPU and memory controller).</p> <p>DRAM-&gt; Represents <code>kepler_container_dram_joules_total</code> metric which describes the total energy spent in DRAM by a container.</p> <p>Other-&gt; Represents <code>kepler_container_other_joules_total</code> metric measures the cumulative energy consumption on other host components besides the CPU and DRAM. Generally, this metric is the host energy consumption (from acpi) less the RAPL Package and DRAM.</p> <p>On Telegraf side:</p> <p>Total PKG current Power-&gt; Represents powerstat_package_current_power_consumptions metrics which showcase Current power consumption of processor package. On Grafana it is the sum of the metrics on both the sockets i.e. powerstat_package_current_power_consumptions of socket 0 + powerstat_package_current_power_consumptions of socket 1.</p> <p>Total DRAM power -&gt; Represents powerstat_package_current_dram_power_consumptions metrics which describes the total energy spent in DRAM of both the sockets.</p> <p>Total Thermal design Power -&gt; Represents powerstat_package_current_thermal_power_consumptions metrics which describes maximum Thermal Design Power (TDP) available for processor package. On Grafana it is the sum of the metrics on both the sockets i.e. powerstat_package_current_thermal\\ _power_consumptions of socket 0 + powerstat_package_current_thermal_power_consumptions of socket 1.</p> <p>Total DRAM Power metrics number on Kepler side and Telegraf side aligns with each other(approximately).</p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#telegraf-ipmi-metric","title":"Telegraf- IPMI metric","text":"<p>On Kepler dashboard, we are also pulling IPMI metrics which show Power Supply Current out %.</p> <p></p>"},{"location":"installation/Kepler-Telegraf-integration-steps/#references","title":"References","text":"<p>[1] https://sustainable-computing.io/</p> <p>[2] https://github.com/influxdata/telegraf</p>"},{"location":"installation/community-operator/","title":"Kepler Community Operator on OpenShift","text":""},{"location":"installation/community-operator/#requirements","title":"Requirements","text":"<p>Before you start make sure you have:</p> <ul> <li>An OCP 4.13 or above cluster running</li> <li>Signed in as <code>kubeadmin</code> or a user with <code>cluster-admin</code> role</li> <li><code>oc</code> installed.</li> <li> <p>Clone the kepler-operator repository.</p> <pre><code>git clone https://github.com/sustainable-computing-io/kepler-operator.git\ncd kepler-operator\n</code></pre> </li> </ul>"},{"location":"installation/community-operator/#remove-previously-installed-version-of-the-kepler-community-operator","title":"Remove previously installed version of the Kepler Community Operator","text":"<p>If you have previously installed the Kepler Community Operator this will need to be removed prior to the installation of the <code>v0.8.z</code> version or above of the operator. This is due to changes to the Kepler API that are backward incompatible. Please also note that <code>v1alpha1</code> does not promise backward compatibility and backward incompatible changes are expected until the API matures to <code>v1beta1</code>.</p> <p>To remove the Kepler Operator use the Uninstall Operator Script in the Kepler-Operator repo</p> <ul> <li> <p>Run the uninstallation script to check the installed version of the operator</p> <pre><code>./hack/uninstall-operator.sh\n</code></pre> <p>Sample output of the command</p> <pre><code> \ud83d\udd14 No operator version specified; finding the installed version\n   \u2705 found kepler-operator csv: clusterserviceversion.operators.coreos.com/kepler-operator.v0.8.1\n   \u2705 kepler-operator version: v0.8.1\n   \u2705 Found - Kepler Operator version: v0.8.1\n\n\n   \ud83d\udd06\ud83d\udd06\ud83d\udd06  Resources of Kepler Operator - v0.8.1  \ud83d\udd06\ud83d\udd06\ud83d\udd06\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\nNAME                     DISPLAY   VERSION   REPLACES                 PHASE\nkepler-operator.v0.8.1   Kepler    0.8.1     kepler-operator.v0.8.0   Succeeded\n\n\n   \ud83d\udd06\ud83d\udd06\ud83d\udd06  Going to delete the following  \ud83d\udd06\ud83d\udd06\ud83d\udd06\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n \u276f kubectl get ns kepler\n\nError from server (NotFound): namespaces \"kepler\" not found\n \u276f kubectl get kepler -A\n\nNAME     PORT   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   AGE\nkepler   9103   17        17        17      17           17          12h\n \u276f kubectl get -n openshift-operators olm -l operators.coreos.com/kepler-operator.openshift-operators=\n\nNAME                                                            AGE\noperatorcondition.operators.coreos.com/kepler-operator.v0.8.1   12h\n....\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n \ud83d\udd14 To delete all resources listed above, rerun with the `--delete` option added.\n\n    \ue795     ./hack/uninstall-operator.sh  --delete\n</code></pre> </li> <li> <p>Once the above is verified, uninstall the operator and all the related resources by specifying the --delete flag.</p> <pre><code>./hack/uninstall-operator.sh  --delete\n</code></pre> </li> </ul>"},{"location":"installation/community-operator/#install-kepler-community-operator-from-operator-hub","title":"Install Kepler Community Operator from Operator Hub","text":"<ul> <li> <p>Go to Operators \u276f Operator Hub. Search for <code>Kepler</code>.   Click on Kepler Operator tile, then select <code>Continue</code> and then <code>Install</code></p> <p></p> </li> <li> <p>Choose <code>alpha</code> channel to deploy the <code>latest</code> version of the Operator.</p> <p>From OCP 4.15 onwards operator can be installed on Namespaces other than <code>openshift-operators</code></p> </li> <li> <p>Click on <code>Install</code></p> <p></p> </li> <li> <p>Wait until Operator gets installed</p> <p></p> <p>Follow the <code>View Operator</code> link to view installed Operators in <code>openshift-operators</code> Namespace or use the UI to navigate to installed operators and select the Kepler Operator.</p> </li> <li> <p>Select <code>Create instance</code> to Create a Custom Resource for Kepler</p> <p></p> </li> <li> <p>There is a <code>Form</code> and <code>YAML</code> view, using the YAML view   provides more detail.</p> <p></p> <p></p> </li> <li> <p>Once Kepler is configured select <code>Create</code>.</p> </li> <li> <p>Check that the Kepler is deployed and available</p> <pre><code>oc get kepler kepler\n</code></pre> <pre><code>NAME     PORT   DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   AGE\nkepler   9103   6         6         6       6            6           33s\n</code></pre> </li> </ul>"},{"location":"installation/community-operator/#kepler-dashboard","title":"Kepler Dashboard","text":"<p>The Kepler Dashboard provides the ability to visualize data exported by Kepler thus facilitating data-driven insights and a clear and interactive overview of metrics. Currently, we can visualize Kepler related metrics either via deploying Grafana dashboard on OpenShift or directly via OpenShift Console.</p>"},{"location":"installation/community-operator/#openshift-console","title":"OpenShift Console","text":"<p>To view the metrics directly from OpenShift Console</p> <ul> <li>Configure user workload monitoring on the cluster. Refer to the official OpenShift documentation for more information.</li> <li>Navigate to Observe \u276f Dashboard</li> <li>To view overall power consumption select <code>Power Monitoring / Overview</code> from the dropdown.     </li> <li>To view the power consumption by namespace select <code>Power Monitoring / Namespace</code> from the dropdown.     </li> </ul>"},{"location":"installation/community-operator/#deploy-the-grafana-dashboard","title":"Deploy the Grafana Dashboard","text":"<p>The Kepler dashboard can be installed using the deploy grafana script in the kepler-operator repo</p> <ul> <li> <p>Run the Grafana deployment script</p> <pre><code>./hack/dashboard/openshift/deploy-grafana.sh\n</code></pre> </li> </ul> <p>The script takes a few minutes to complete. The script automates the following steps:</p> <ul> <li>Setup OpenShift User Workload Monitoring.</li> <li>Install the Grafana Community Operator inside <code>kepler-grafana</code> namespace</li> <li>Setup Grafana related dependencies e.g. ServiceAccount, Grafana DataSource, Grafana Dashboard and Route</li> </ul> <p>When the script successfully completes it provides the OpenShift Route to the Kepler Dashboard.</p> <pre><code>   \ud83d\udd06\ud83d\udd06\ud83d\udd06  Grafana Dashboard Setup Complete  \ud83d\udd06\ud83d\udd06\ud83d\udd06\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n  \ud83d\udcc8 Grafana Configuration:\n\n   Dashboard URL: https://grafana-route-kepler-grafana.apps.devcluster.openshift.com/login\n           Admin: kepler\n        Password: kepler\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n \ud83d\udd14 Kepler use prometheus deployed in openshift-user-workload-monitoring to store metrics. To configure Prometheus to cater to needs of the cluster such as:\n\n    * Increase data retention for in-depth analysis\n    * Allocate more resources based on requirements\n\n\ud83d\udca1 see: https://docs.openshift.com/container-platform/latest/observability/monitoring/configuring-the-monitoring-stack.html#configuring-the-monitoring-stack_configuring-the-monitoring-stack\n\n\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\n</code></pre>"},{"location":"installation/community-operator/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>Sign in to the Grafana dashboard using the credentials <code>kepler:kepler</code>.</p> <p></p>"},{"location":"installation/community-operator/#access-the-grafana-console-route","title":"Access the Grafana Console Route","text":"<p>The dashboard can also be accessed through the OCP UI, Go to Networking \u276f Routes.</p> <p></p>"},{"location":"installation/community-operator/#grafana-deployment-overview","title":"Grafana Deployment Overview","text":"<p>Refer to the Grafana Deployment Overview</p> <p></p>"},{"location":"installation/community-operator/#frequently-asked-questions","title":"Frequently Asked Questions","text":""},{"location":"installation/community-operator/#will-kepler-work-on-earlier-releases-of-openshift","title":"Will Kepler work on earlier releases of OpenShift?","text":"<p>Our recommendation is use <code>OCP 4.13</code> and above, but Kepler can be installed on <code>OCP 4.11</code> and <code>4.12</code>. In the future the Operator may be updated to check the version of Kubernetes that is installed e.g. <code>v1.25</code>.</p>"},{"location":"installation/community-operator/#how-do-i-set-nodeselector-and-tolerations-for-kepler","title":"How do I set nodeSelector and tolerations for Kepler?","text":"<p>You can specify nodeSelector and toleration's for Kepler at the time of creating Instance. You can specify both in <code>Form</code> and <code>YAML</code> view.</p> <p>To specify in <code>YAML</code> view:</p> <pre><code>spec:\nexporter:\n  deployment:\n    port: 9103\n    nodeSelector:\n      foo: bar\n    tolerations:\n      - key: foo\n        operator: \"Equal\"\n        value: bar\n        effect: NoExecute\n</code></pre>"},{"location":"installation/community-operator/#how-do-i-specify-redfish-related-configuration","title":"How do I specify Redfish related configuration?","text":"<p>You can specify Redfish related configuration for Kepler at the time of creating Instance. You can specify both in <code>Form</code> and <code>YAML</code> view.</p> <p>To specify in <code>YAML</code> view:</p> <pre><code>spec:\n  exporter:\n    deployment:\n      port: 9103\n      tolerations:\n        - operator: Exists\n    redfish:\n      secretRef: redfish-secret\n      probeInterval: 60s\n      skipSSLVerify: false\n</code></pre> <p>Note</p> <p>Once an instance is created, the user must manually create redfish secret <code>redfish-secret</code> in the namespace <code>kepler-operator</code>. Once the secret is created, the operator will reconcile and Kepler will be able to connect to Redfish.</p> <p>For more information regarding secret content specification refer to the upstream manifest.</p>"},{"location":"installation/community-operator/#where-are-kepler-exporter-pods-deployed","title":"Where are Kepler exporter pods deployed?","text":"<p>Once a Kepler Instance is created all related resources like pods, daemonsets, configmaps, secrets etc. are present inside the <code>kepler-operator</code> namespace. To view the available resources:</p> <pre><code>oc get all -n kepler-operator\n</code></pre>"},{"location":"installation/kepler-helm/","title":"Deploy using Helm Chart","text":"<p>The Kepler Helm Chart is available on GitHub and ArtifactHub</p>"},{"location":"installation/kepler-helm/#install-helm","title":"Install Helm","text":"<p>Helm must be installed to use the charts. Please refer to Helm's documentation to get started.</p>"},{"location":"installation/kepler-helm/#prometheus-setup","title":"Prometheus Setup","text":"<p>The Kepler Exporter requires the Prometheus Node Exporter to be installed. We recommend the Kube Prometheus Stack helm chart, which includes the Node Exporter, Grafana and other helpful stuff to provide easy to operate end-to-end Kubernetes cluster monitoring with Prometheus using the Prometheus Operator.</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\n    --namespace monitoring \\\n    --create-namespace \\\n    --wait\n</code></pre>"},{"location":"installation/kepler-helm/#add-the-kepler-helm-repo","title":"Add the Kepler Helm repo","text":"<pre><code>helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart\nhelm repo update\n</code></pre> <p>You can see the latest version by using the following command:</p> <pre><code>helm search repo kepler\n</code></pre> <p>If you would like to test and look at the manifest files before deploying you can run:</p> <pre><code>helm install kepler kepler/kepler --namespace kepler --create-namespace --dry-run --devel\n</code></pre>"},{"location":"installation/kepler-helm/#install-kepler","title":"Install Kepler","text":"<p>For Prometheus to be able to discover the metrics exported by Kepler, the <code>serviceMonitor</code> needs to be enabled and labeled with the release name of your Prometheus install. In our installation we called our kube-prometheus-stack install <code>prometheus</code>:</p> <pre><code>helm install kepler kepler/kepler \\\n    --namespace kepler \\\n    --create-namespace \\\n    --set serviceMonitor.enabled=true \\\n    --set serviceMonitor.labels.release=prometheus \\\n</code></pre> <p>Alternatively, you can also override the values.yaml file to set these and the following values:</p> <pre><code>helm install kepler kepler/kepler --values values.yaml --namespace kepler --create-namespace\n</code></pre> <p>The following table lists the configurable parameters for this chart and their default values.</p> Parameter Description Default global.namespace Kubernetes namespace for Kepler kepler image.repository Repository for Kepler Image quay.io/sustainable_computing_io/kepler image.pullPolicy Pull policy for Kepler Always image.tag Image tag for Kepler Image latest serviceAccount.name Service account name for Kepler kepler-sa service.type Kepler service type ClusterIP service.port Kepler service exposed port 9102"},{"location":"installation/kepler-helm/#post-install","title":"Post Install","text":"<p>After Installation you can wait for Kepler to get ready:</p> <pre><code>KPLR_POD=$(\n    kubectl get pod \\\n        -l app.kubernetes.io/name=kepler \\\n        -o jsonpath=\"{.items[0].metadata.name}\" \\\n        -n kepler\n)\nkubectl wait --for=condition=Ready pod $KPLR_POD --timeout=-1s -n kepler\n</code></pre> <p>and add the Kepler dashboard to Grafana:</p> <pre><code>GF_POD=$(\n    kubectl get pod \\\n        -n monitoring \\\n        -l app.kubernetes.io/name=grafana \\\n        -o jsonpath=\"{.items[0].metadata.name}\"\n)\nkubectl cp kepler_dashboard.json monitoring/$GF_POD:/tmp/dashboards/kepler_dashboard.json\n</code></pre>"},{"location":"installation/kepler-helm/#uninstall-kepler","title":"Uninstall Kepler","text":"<p>To uninstall this chart, use the following steps</p> <pre><code>helm delete kepler --namespace kepler\n</code></pre>"},{"location":"installation/kepler-operator/","title":"Kepler Operator on Kind","text":""},{"location":"installation/kepler-operator/#requirements","title":"Requirements","text":"<p>Before you start make sure you have:</p> <ul> <li><code>docker</code> installed and configured to run as non-root by default</li> <li><code>kubectl</code> installed</li> <li><code>kind</code> installed</li> <li>Clone the <code>kepler-operator</code> repository</li> <li>Sign in as <code>kubeadmin</code> or a user with <code>cluster-admin</code> role</li> </ul> <p>Note</p> <p>Your controller will automatically use the current context in your kubeconfig file (i.e. whatever cluster kubectl cluster-info shows).</p>"},{"location":"installation/kepler-operator/#run-a-kind-cluster-locally","title":"Run a kind cluster locally","text":"<pre><code>cd kepler-operator\nmake cluster-up\n</code></pre>"},{"location":"installation/kepler-operator/#run-kepler-operator","title":"Run kepler-operator","text":"<ul> <li> <p>You can use the image from quay.io to deploy kepler-operator.</p> <pre><code>make deploy OPERATOR_IMG=quay.io/sustainable_computing_io/kepler-operator:[VERSION]\nkubectl apply -k config/samples/\n</code></pre> </li> <li> <p>Alternatively, if you like to setup cluster, build and use your own images</p> <pre><code>make fresh\n</code></pre> <p>The above will create a kind cluster build and push operator and bundle images to local registry.</p> </li> </ul>"},{"location":"installation/kepler-operator/#set-up-grafana-dashboard","title":"Set up Grafana Dashboard","text":"<p>Using <code>GRAFANA_ENABLE=true</code> and <code>PROMETHEUS_ENABLE=true</code> when running <code>make cluster-up</code> configures the <code>kube-prometheus</code> monitoring stack in the namespace <code>monitoring</code>. To access the Grafana Console locally on the browser port-forward on 3000 using the following command:</p> <pre><code>kubectl port-forward svc/grafana 3000:3000 -n monitoring\n</code></pre> <p>Note</p> <p>Grafana Console can be accessed on http://localhost:3000</p>"},{"location":"installation/kepler-operator/#service-monitor","title":"Service Monitor","text":"<p>For <code>kube-prometheus</code> to scrape <code>kepler-exporter</code> service endpoint you need to configure a service monitor.</p> <p>Note</p> <p>By default <code>kube-prometheus</code> does not let you scrape services deployed in namespaces other than <code>monitoring</code>. So if you are running Kepler outside <code>monitoring</code> follow this to set up Prometheus to scrape all namespaces.</p> <pre><code>kubectl apply -n monitoring -f - &lt;&lt; EOF\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kepler-exporter\n    sustainable-computing.io/app: kepler\n  name: monitor-kepler-exporter\nspec:\n  endpoints:\n  - interval: 3s\n    port: http\n    relabelings:\n    - action: replace\n      regex: (.*)\n      replacement: $1\n      sourceLabels:\n      - __meta_kubernetes_pod_node_name\n      targetLabel: instance\n    scheme: http\n  jobLabel: app.kubernetes.io/name\n  namespaceSelector:\n    matchNames:\n    any: true\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kepler-exporter\nEOF\n</code></pre>"},{"location":"installation/kepler-operator/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>To set up the Grafana dashboard follow these steps:</p> <ul> <li>Sign in localhost:3000 using <code>admin:admin</code></li> <li> <p>Import default dashboard from Kepler operator repository</p> <p></p> </li> </ul>"},{"location":"installation/kepler-operator/#uninstall-the-operator","title":"Uninstall the operator","text":"<p>To delete the CRDs, roles, roleBindings etc from the cluster:</p> <pre><code>make undeploy\n</code></pre>"},{"location":"installation/kepler-operator/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/kepler-operator/#scrape-all-namespaces","title":"Scrape all namespaces","text":"<p>kube-prometheus by default does not let you scrape services in namespaces outside <code>monitoring</code>. This is controlled by RBAC. The clusterrole <code>prometheus-k8s</code> should have below policy to scrape services in all namespaces.</p> <pre><code>kubectl describe clusterrole prometheus-k8s\nName:         prometheus-k8s\nLabels:       app.kubernetes.io/component=prometheus\n              app.kubernetes.io/instance=k8s\n              app.kubernetes.io/name=prometheus\n              app.kubernetes.io/part-of=kube-prometheus\n              app.kubernetes.io/version=2.45.0\nAnnotations:  &lt;none&gt;\nPolicyRule:\n  Resources                    Non-Resource URLs  Resource Names  Verbs\n  ---------                    -----------------  --------------  -----\n  endpoints                    []                 []              [get list watch]\n  pods                         []                 []              [get list watch]\n  services                     []                 []              [get list watch]\n  ingresses.networking.k8s.io  []                 []              [get list watch]\n                               [/metrics]         []              [get]\n  nodes/metrics                []                 []              [get]\n</code></pre> <ul> <li> <p>To customize the Prometheus after creating local cluster follow kube-prometheus documentation on Customizing Kube-Prometheus</p> </li> <li> <p>Make sure you apply this jsonnet to ensure Prometheus scrapes services in all namespaces.</p> </li> </ul>"},{"location":"installation/kepler-rpm/","title":"Install Kepler as RPM","text":""},{"location":"installation/kepler-rpm/#versions-0710-and-newer","title":"Versions 0.7.10 and newer","text":"<p>The current rpm release is a systemd unit that starts a podman container.</p> <p>Download the latest stable release from the Kepler release URL download</p> <pre><code>tar xvzf kepler.rpm.tar.gz\nyum install RPMS/noarch/container-kepler-0.7.10-1.noarch.rpm\nsystemctl enable container-kepler --now\n</code></pre> <p>Verify that podman starts a kepler container via</p> <p><code>sudo podman ps</code></p> <p>then via your browser pointing to the URL below on the preconfigured port browser</p> <p>or via a curl command:</p> <p><code>curl localhost:8888/metrics | grep kepler_node_package_joules_total</code></p>"},{"location":"installation/kepler-rpm/#versions-prior-to-0710","title":"Versions prior to 0.7.10","text":"<p>Older version directly install kepler as a rpm package (as opposed to as a container in newer versions) To install the Kepler RPM download the latest stable version, unpack and install:</p> <pre><code>sudo dnf localinstall kepler-[version.arch].rpm\n\nsystemctl start kepler.service\n</code></pre> <p>Check status with</p> <pre><code>systemctl status kepler.service\n\njournalctl -f | grep kepler\n</code></pre> <p>In order to do process-level energy accounting type:</p> <pre><code>mkdir -p /etc/kepler/kepler.config\necho -n true &gt; /etc/kepler/kepler.config/ENABLE_PROCESS_METRICS\n</code></pre> <p>The kepler service runs on default port 8888.</p> <p>Use your web browser to navigate to the machine IP on port 8888.</p>"},{"location":"installation/kepler/","title":"Deploy using Manifests","text":""},{"location":"installation/kepler/#getting-started","title":"Getting Started","text":"<p>The following instructions work for both <code>Kind</code> and <code>Kubeadm</code> clusters.</p>"},{"location":"installation/kepler/#prerequisites","title":"Prerequisites","text":"<ol> <li> <p>You have a Kubernetes cluster running.</p> <p>Note</p> <p>If you want to setup a kind cluster follow this</p> </li> <li> <p>The Monitoring stack, i.e. Prometheus with Grafana is set up. Steps here</p> <p>Note</p> <p>The default Grafana deployment can be accessed with the credentials <code>admin:admin</code>. You can expose the web-based UI locally using:</p> <pre><code>kubectl -n monitoring port-forward svc/grafana 3000\n</code></pre> </li> </ol> <p>If the perquisites are met, then please proceed to the following sections.</p>"},{"location":"installation/kepler/#deploying-kepler-on-a-local-kind-cluster","title":"Deploying Kepler on a local kind cluster","text":"<p>To deploy Kepler on <code>kind</code>, we need to build it locally with specific flags. The full details of local builds are covered in the section below. To deploy on a local <code>kind</code> cluster, you need to build using the <code>PROMETHEUS_DEPLOY</code> flag.</p> <pre><code>git clone --depth 1 git@github.com:sustainable-computing-io/kepler.git\ncd ./kepler\nmake build-manifest OPTS=\"PROMETHEUS_DEPLOY\"\nkubectl apply -f _output/generated-manifest/deployment.yaml\n</code></pre>"},{"location":"installation/kepler/#deploying-kepler-on-a-baremetal-kubeadm-cluster","title":"Deploying Kepler on a baremetal Kubeadm cluster","text":"<p>To deploy Kepler on Kubeadm, we need to build it locally with specific flags. The full details of local builds are covered in the section below. To deploy on a local <code>Kubeadm</code> cluster, you need to use the <code>BM_DEPLOY</code> and <code>PROMETHEUS_DEPLOY</code> flags.</p> <pre><code>git clone --depth 1 git@github.com:sustainable-computing-io/kepler.git\ncd ./kepler\nmake build-manifest OPTS=\"BM_DEPLOY PROMETHEUS_DEPLOY\"\nkubectl apply -f _output/generated-manifest/deployment.yaml\n</code></pre>"},{"location":"installation/kepler/#dashboard-access","title":"Dashboard access","text":"<p>The deployment steps above will create a Kepler service listening on port <code>9102</code>.</p> <p>If you followed the Kepler dashboard deployment steps, you can access the Kepler dashboard by navigating to http://localhost:3000/ Login using <code>admin:admin</code>. Skip the window where Grafana asks to input a new password.</p> <p></p> <p>Note</p> <p>To forward ports simply run: <pre><code>kubectl port-forward --address localhost -n kepler service/kepler-exporter 9102:9102 &amp;\nkubectl port-forward --address localhost -n monitoring service/prometheus-k8s 9090:9090 &amp;\nkubectl port-forward --address localhost -n monitoring service/grafana 3000:3000 &amp;\n</code></pre></p>"},{"location":"installation/kepler/#build-manifests","title":"Build manifests","text":"<p>First, fork the kepler repository and clone it.</p> <p>If you want to use Redfish BMC and IPMI, you need to add Redfish and IPMI credentials of each of the kubelet node to the <code>redfish.csv</code> under the <code>kepler/manifests/k8s/config/exporter</code> directory. The format of the file is as follows:</p> <pre><code>kubelet_node_name_1,redfish_username_1,redfish_password_2,https://redfish_ip_or_hostname_1\nkubelet_node_name_2,redfish_username_2,redfish_password_2,https://redfish_ip_or_hostname_2\n</code></pre> <p>where, <code>kubelet_node_name</code> in the first column is the name of the node where the kubelet is running. You can get the name of the node by running the following command:</p> <pre><code>kubectl get nodes\n</code></pre> <p><code>redfish_username</code> and <code>redfish_password</code> in the second and third columns are the credentials to access the Redfish API from each node. While <code>https://redfish_ip_or_hostname</code> in the fourth column is the Redfish endpoint in IP address or hostname.</p> <p>Then, build the manifests file that suit your environment and deploy it with the following steps:</p> <pre><code>make build-manifest OPTS=\"&lt;deployment options&gt;\"\n</code></pre> <p>Minimum deployment:</p> <pre><code>make build-manifest\n</code></pre> <p>Deployment with sidecar on openshift:</p> <pre><code>make build-manifest OPTS=\"ESTIMATOR_SIDECAR_DEPLOY OPENSHIFT_DEPLOY\"\n</code></pre> <p>Manifests will be generated in  <code>_output/generated-manifest/</code> by default.</p> Deployment Option Description Dependency BM_DEPLOY baremetal deployment patched with node selector feature.node.kubernetes.io/cpu-cpuid.HYPERVISOR to not exist - OPENSHIFT_DEPLOY patch openshift-specific attribute to kepler daemonset and deploy SecurityContextConstraints - PROMETHEUS_DEPLOY patch prometheus-related resource (ServiceMonitor, RBAC role, rolebinding) require prometheus deployment which can be OpenShift integrated or custom deploy HIGH_GRANULARITY sets the Prometheus scrape interval for Kepler to 3s (default is 30s) PROMETHEUS_DEPLOY option set CLUSTER_PREREQ_DEPLOY deploy prerequisites for kepler on openshift cluster OPENSHIFT_DEPLOY option set CI_DEPLOY update proc path for kind cluster using in CI - ESTIMATOR_SIDECAR_DEPLOY patch estimator sidecar and corresponding ConfigMap to kepler daemonset - MODEL_SERVER_DEPLOY deploy model server and corresponding ConfigMap to kepler daemonset - TRAINER_DEPLOY patch online-trainer sidecar to model server MODEL_SERVER_DEPLOY option set DEBUG_DEPLOY patch KEPLER_LOG_LEVEL for debugging - QAT_DEPLOY update proc path for Kepler to enable accelerator QAT Intel QAT installed DCGM_DEPLOY Enable <code>hostNetwork: true</code> in Kepler container to access local DCGM service; use <code>latest-dcgm</code> Kepler container image to load DCGM library and dependencies NVIDIA DCGM service must be installed on the node <p>Following options are available for Redfish client, you can set them as environment variables of kepler-exporter. They affect all of Redfish access from Kepler Exporter.</p> Option Default value Description REDFISH_PROBE_INTERVAL_IN_SECONDS 60 Interval in seconds to get power consumption via Redfish. REDFISH_SKIP_SSL_VERIFY true <code>true</code> if TLS verification is disabled on connecting to Redfish endpoint. <p><code>build-manifest</code> requirements:</p> <ul> <li>kubectl v1.21+</li> <li>make</li> <li>go</li> </ul>"},{"location":"installation/kepler/#deploy-the-prometheus-operator","title":"Deploy the Prometheus operator","text":"<p>If Prometheus is already installed in the cluster, skip this step. Otherwise, follow these steps to install it.</p> <ol> <li> <p>Clone the kube-prometheus project to   your local folder, and enter the <code>kube-prometheus</code> directory.</p> <pre><code>git clone --depth 1 https://github.com/prometheus-operator/kube-prometheus; cd kube-prometheus;\n</code></pre> </li> <li> <p>This step is optional. You can later manually add the Kepler Grafana dashboard through the   Grafana UI. To automatically do that, fetch the <code>kepler-exporter</code> Grafana dashboard and inject in   the Prometheus Grafana deployment.</p> <pre><code>$ KEPLER_EXPORTER_GRAFANA_DASHBOARD_JSON=`curl -fsSL https://raw.githubusercontent.com/sustainable-computing-io/kepler/main/grafana-dashboards/Kepler-Exporter.json | sed '1 ! s/^/         /'`\n$ mkdir -p grafana-dashboards\n$ cat - &gt; ./grafana-dashboards/kepler-exporter-configmap.yaml &lt;&lt; EOF\napiVersion: v1\ndata:\n    kepler-exporter.json: |-\n        $KEPLER_EXPORTER_GRAFANA_DASHBOARD_JSON\nkind: ConfigMap\nmetadata:\n    labels:\n        app.kubernetes.io/component: grafana\n        app.kubernetes.io/name: grafana\n        app.kubernetes.io/part-of: kube-prometheus\n        app.kubernetes.io/version: 9.5.3\n    name: grafana-dashboard-kepler-exporter\n    namespace: monitoring\nEOF\n</code></pre> <p>Note</p> <p>The next step uses yq, a YAML processor.</p> <pre><code>yq -i e '.items += [load(\"./grafana-dashboards/kepler-exporter-configmap.yaml\")]' ./manifests/grafana-dashboardDefinitions.yaml\nyq -i e '.spec.template.spec.containers.0.volumeMounts += [ {\"mountPath\": \"/grafana-dashboard-definitions/0/kepler-exporter\", \"name\": \"grafana-dashboard-kepler-exporter\", \"readOnly\": false} ]' ./manifests/grafana-deployment.yaml\nyq -i e '.spec.template.spec.volumes += [ {\"configMap\": {\"name\": \"grafana-dashboard-kepler-exporter\"}, \"name\": \"grafana-dashboard-kepler-exporter\"} ]' ./manifests/grafana-deployment.yaml\n</code></pre> </li> <li> <p>Finally, apply the objects in the <code>manifests</code> directory. This will create the <code>monitoring</code> namespace and CRDs, and then wait for them to be available before creating the remaining resources. During the <code>until</code> loop, a response of <code>No resources found</code> is to be expected. This statement checks whether the resource API is created but doesn't expect the resources to be there.</p> <pre><code>kubectl apply --server-side -f manifests/setup\nuntil kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo \"\"; done\nkubectl apply -f manifests/\n</code></pre> <p>Note</p> <p>It takes a short time (in a Kind cluster), for all the pods and services to reach a <code>running</code> state.</p> </li> </ol>"},{"location":"installation/local-cluster/","title":"Local cluster setup","text":"<p>Kepler runs on Kubernetes. If you already have access to a cluster, you can skip this section. To deploy a local cluster, you can use kind. <code>kind</code> is a tool for running local Kubernetes clusters using Docker container \"nodes\". It was primarily designed for testing Kubernetes itself, but may be used for local development or CI.</p>"},{"location":"installation/local-cluster/#install-kind","title":"Install kind","text":"<p>To install <code>kind</code>, please see the instructions here.</p> <p>We need to configure our cluster to run Kepler. Specifically, we need to mount <code>/proc</code> (to expose information about processes running on the host) and <code>/usr/src</code> (to expose kernel headers allowing dynamic eBPF program compilation - this dependency might be removed in future releases into the node containers. Below is a minimal single-node example configuration:</p> <pre><code>$ cat - &gt; ./local-cluster-config.yaml &lt;&lt; EOF\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nname: my-cluster\nnodes:\n- role: control-plane\n  image: kindest/node:v1.27.3@sha256:3966ac761ae0136263ffdb6cfd4db23ef8a83cba8a463690e98317add2c9ba72\n  extraMounts:\n  - hostPath: /proc\n    containerPath: /proc-host\n  - hostPath: /usr/src\n    containerPath: /usr/src\nEOF\n</code></pre> <p>We can then spin up a cluster with either:</p> <pre><code>export CLUSTER_NAME=\"my-cluster\"  # we can use the --name flag to override the name in our config\nkind create cluster --name=$CLUSTER_NAME --config=./local-cluster-config.yaml\n</code></pre> <p>or simply by running:</p> <pre><code>make cluster-up\n</code></pre> <p>Note that <code>kind</code> automatically switches your current <code>kubeconfig</code> context to the newly created cluster.</p>"},{"location":"installation/strategy/","title":"Installation Strategies","text":"<p>While you are free to explore any deployments but the recommended strategies are :</p> OCP 4.13 Microshift RHEL ROSA Kind kepler-operator Manifests RPM Manifests Helm Charts, Manifests, kepler-operator"},{"location":"installation/strategy/#requirements","title":"Requirements","text":"<ul> <li>Kernel 4.18+</li> <li><code>kubectl</code> v1.21.0+</li> <li><code>docker</code> installed non-root by default</li> </ul>"},{"location":"kepler_model_server/api/","title":"Kepler Model Server API","text":""},{"location":"kepler_model_server/api/#getting-powers-from-estimator","title":"Getting Powers from Estimator","text":"<p>module: estimator (src/estimate/estimator.py)</p> <pre><code>/tmp/estimator.socket\n</code></pre>"},{"location":"kepler_model_server/api/#parameters-of-powerrequest","title":"Parameters of PowerRequest","text":"key value description metrics list of string list of available input features (measured metrics) output_type either of the following values: AbsPower (for node-level power model), DynPower (for container-level power model) the requested model type trainer_name (optional) string filter model with trainer name filter (optional) string expression in the form attribute1:threshold1; attribute2:threshold2"},{"location":"kepler_model_server/api/#getting-power-models-from-model-server","title":"Getting Power Models from Model Server","text":"<p>module: server (src/server/model_server.py)</p> <pre><code>:8100/model\nPOST\n</code></pre>"},{"location":"kepler_model_server/api/#parameters-of-modelrequest","title":"Parameters of ModelRequest","text":"key value description metrics list of string list of available input features (measured metrics) output_type either of the following values: AbsPower (for node-level power model), DynPower (for container-level power model) the requested model type weight boolean return model weights in json format if true. Otherwise, return model in zip file format. trainer_name (optional) string filter model with trainer name. node_type (optional) string filter model with node type. filter (optional) string expression in the form attribute1:threshold1; attribute2:threshold2."},{"location":"kepler_model_server/api/#offline-trainer","title":"Offline Trainer","text":"<p>module: offline trainer (src/train/offline_trainer.py)</p> <pre><code>:8102/train\nPOST\n</code></pre>"},{"location":"kepler_model_server/api/#parameters-of-trainrequest","title":"Parameters of TrainRequest","text":"key value description name string pipeline/model name energy_source valid key in PowerSourceMap target energy source to train for trainer TrainAttribute attributes for training prome_response json prom response with workload for power model training"},{"location":"kepler_model_server/api/#trainattribute","title":"TrainAttribute","text":"key value description abs_trainers list of available trainer class names trainer classes in the pipeline to train for absolute power dyn_trainers list of available trainer class names trainer classes in the pipeline to train for dynamic power isolator valid isolator class name isolator class of the pipeline to isolate the target data to train for dynamic power isolator_args dict mapping between isolator-specific argument name and value"},{"location":"kepler_model_server/api/#posting-model-weights-wip","title":"Posting Model Weights [WIP]","text":"<p>module: server (src/server/model_server.py)</p> <pre><code>/metrics\nGET\n</code></pre>"},{"location":"kepler_model_server/api/#online-trainer-wip","title":"Online Trainer [WIP]","text":"<p>module: online trainer (src/train/online_trainer.py) running as a sidecar to server</p> <pre><code>periodically query prometheus metric server on SAMPLING INTERVAL\n</code></pre>"},{"location":"kepler_model_server/api/#profiler-wip","title":"Profiler [WIP]","text":"<p>module: profiler (src/profile/profiler.py)</p>"},{"location":"kepler_model_server/architecture/","title":"Kepler Model Server Architecture","text":"<p>Kepler model server is a supplementary project of Kepler that facilitates power model training and serving. This provides an ecosystem of Kepler to collect metrics from one environment, train a power model with pipeline framework, and serve back to another environment that a power meter (energy measurement) is not available.</p> <p></p> <p>Pipeline Input: Prometheus query results during the training workload war running.</p> <p>Pipeline Output: A directory that contains archived absolute and dynamic power models trained by each available feature group which is labeled by each available energy source.</p> <pre><code>[Pipeline name]/[Energy source]/[Model type]/[Feature group]/[Archived model]\n</code></pre> <ul> <li>Pipeline name a unique name for different composition of modeling approach such as different extractor, isolator, set of trainers, supported feature groups, and supported energy sources.</li> <li>Energy/Power source a power meter source of power label.</li> <li>Model type a type of model with or without background isolation.</li> <li>Feature group a utilization metric source of model input.</li> <li>Archived model a folder and zip file in the format<code>[trainer name]_[node type]</code> where trainer is a name of training solution such as <code>GradientBoostingRegressor</code> and <code>node_type</code> is a categorized profile of the server used for training. The folder contains</li> <li>metadata.json</li> <li>model files</li> <li>weight.json (model weight for local estimator supported models such as linear regression (LR))</li> <li>feature engineering (fe) files</li> </ul> <p>Check out the project on GitHub \u27a1\ufe0f Kepler Model Server.</p>"},{"location":"kepler_model_server/get_started/","title":"Get Started with Kepler Model Server","text":"<p>Model server project facilitates tools for power model training, exporting, serving, and utilizing based on Kepler-exporting energy-related metrics. Check the following steps to get started with the project.</p>"},{"location":"kepler_model_server/get_started/#step-1-learn-about-pipeline","title":"Step 1: Learn about Pipeline","text":"<p>The first step is to understand about power model building concept from training pipeline.</p>"},{"location":"kepler_model_server/get_started/#step-2-learn-how-to-use-the-power-model","title":"Step 2: Learn how to use the power model","text":""},{"location":"kepler_model_server/get_started/#select-estimator","title":"Select estimator","text":"<p>There are two ways to use the models regarding the model format. If the model format can be processed directly inside the Kepler exporter such as Linear Regression weight in <code>json</code> format. There is no extra cofiguration.</p> <p>However, if the model is in the general format archived in <code>zip</code>, It is needed to enable the estimator sidecar via environment variable or Kepler config map.</p> <pre><code>export NODE_COMPONENTS_ESTIMATOR=true\n</code></pre> <p>or</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kepler-cfm\nnamespace: kepler\ndata:\n    MODEL_CONFIG: |\n        NODE_COMPONENTS_ESTIMATOR=true\n</code></pre>"},{"location":"kepler_model_server/get_started/#select-power-model","title":"Select power model","text":"<p>There are two ways to obtain power model: static and dynamic.</p>"},{"location":"kepler_model_server/get_started/#static-configuration","title":"Static configuration","text":"<p>A static way is to download the model directly from <code>INIT_URL</code>. It can be set via environment variable directly or via <code>kepler-cfm</code> Kepler config map. For example,</p> <pre><code>export NODE_COMPONENTS_INIT_URL= &lt; Static URL &gt;\n</code></pre> <p>or</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kepler-cfm\nnamespace: kepler\ndata:\n    MODEL_CONFIG: |\n        NODE_COMPONENTS_INIT_URL= &lt; Static URL &gt;\n</code></pre> <p>The static URL from provided pipeline v0.7 are listed here.</p>"},{"location":"kepler_model_server/get_started/#dynamic-via-server-api","title":"Dynamic via server API","text":"<p>A dynamic way is to enable the model server to auto select the power model which has the best accuracy and supported the running cluster environment. Similarly, It can be set via the environment variable or set it via Kepler config map.</p> <pre><code>export MODEL_SERVER_ENABLE=true\n</code></pre> <p>or</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\nname: kepler-cfm\nnamespace: kepler\ndata:\n    MODEL_CONFIG: |\n        MODEL_SERVER_ENABLE: \"true\"\n</code></pre> <p>See more in Kepler Power Estimation Deployment</p>"},{"location":"kepler_model_server/get_started/#step-3-learn-how-to-train-the-power-model-and-give-back-to-the-community","title":"Step 3: Learn how to train the power model and give back to the community","text":"<p>As you may be aware, it's essential to tailor power models to specific machine types rather than relying on a single generic model. We eagerly welcome contributions from the community to help build alternative power models on your machine through the model server project.</p> <p>For detailed guidance on model training, please refer to our model training guidelines here.</p>"},{"location":"kepler_model_server/node_profile/","title":"Node Profile","text":"<p>We form a group of machines (nodes) called node type based on processor model, the number of cores, the number of chips, memory size, and maximum CPU frequency. When collecting the data from the bare metal machine, these attributes are automatically extracted and kept as a machine spec in json format.</p> <p>A power model will be built per node type. For each group of node type, we make a profile composing of background power when the resource usage is almost constant without user workload, minimum, maximum power for each power components (e.g., core, uncore, dram, package, platform), and normalization scaler (i.e., MinMaxScaler), standardization scaler (i.e., StandardScaler) for each feature group.</p> <p>Node specification is composed of:</p> <ul> <li>processor - CPU processor model name</li> <li>cores - Number of CPU cores</li> <li>chips - Number of chips</li> <li>memory_gb - Memory size in GB</li> <li>cpu_freq_mhz - Maximum CPU frequency in MHz</li> </ul>"},{"location":"kepler_model_server/pipeline/","title":"Training Pipeline","text":"<p>Model server can provide various power models for different context and learning methods. Training pipeline is an abstract of power model training that applies a set of learning methods to a different combination of energy sources, power isolation methods, available energy-related metrics.</p>"},{"location":"kepler_model_server/pipeline/#pipeline","title":"Pipeline","text":"<p><code>pipeline</code> is composed of three steps, <code>extract</code>, <code>isolate</code>, and <code>train</code>, as shown below. Kepler exports energy-related metrics as Prometheus counter, which provides accumulated number over time.</p> <p>The <code>extract</code> step is to convert the counter metrics to gauge metrics, similarly to the Prometheus <code>rate()</code> function, giving per-second values. The extract step also clean up the data separately for each <code>feature group</code>.</p> <p>The power consumption retrieved from the Prometheus query is the measured power which is composed of the power portion which is varied by the workload called dynamic power and the power portion which is consumed even if in the idling state called idle power.</p> <p>The <code>isolate</code> step is to calculate the idle power and isolate the dynamic power consumption of each <code>energy source</code>.</p> <p>The <code>train</code> step is to apply each <code>trainer</code> to create multiple choices of power models based on the preprocessed data.</p> <p>We have a roadmap to apply a pipeline to build power models separately for each node/machine type. Find more in Node Type section.</p> <p></p> <ul> <li>Learn more about <code>energy source</code> from Energy source section.</li> <li>Learn more about <code>feature group</code> from Feature groups section.</li> <li>Learn more about the <code>isolate</code> step and corresponding concepts of <code>AbsPower</code>, and <code>DynPower</code> power models from Power isolation section.</li> <li>Check available <code>trainer</code> in Trainer section.</li> </ul>"},{"location":"kepler_model_server/pipeline/#energy-source","title":"Energy source","text":"<p><code>energy source</code> or <code>source</code> refers to the source (power meter) that provides an energy number. Each source provides one or more <code>energy components</code>. Currently supported source are shown as below.</p> Energy/power source Energy/power components rapl package, core, uncore, dram acpi platform"},{"location":"kepler_model_server/pipeline/#feature-group","title":"Feature group","text":"<p><code>feature group</code> is an abstraction of the available features based on the infrastructure context since some environments might not expose some metrics. For example, on the virtual machine in private cloud environment, hardware counter metrics are typically not available. Therefore, the models are trained for each defined resource utilization metric group as below.</p> Group Name Features Kepler Metric Source(s) CounterOnly COUNTER_FEATURES Hardware Counter BPFOnly BPF_FEATURES BPF IRQOnly IRQ_FEATURES IRQ AcceleratorOnly ACCELERATOR_FEATURES Accelerator CounterIRQCombined COUNTER_FEATURES, IRQ_FEATURES BPF and Hardware Counter Basic COUNTER_FEATURES, BPF_FEATURES All except IRQ and node information WorkloadOnly COUNTER_FEATURES, BPF_FEATURES, IRQ_FEATURES, ACCELERATOR_FEATURES All except node information Full WORKLOAD_FEATURES, SYSTEM_FEATURES All <p>Node information refers to value from kepler_node_info metric.</p>"},{"location":"kepler_model_server/pipeline/#power-isolation","title":"Power isolation","text":"<p>The power consumption retrieved from the Prometheus query is the absolute power, which is the sum of idle and dynamic power (where idle represents the system at rest, dynamic is the incremental power with resource utilization, and absolute is idle + dynamic). Additionally, this power is also the total power consumption of all process, including the users' workload, background and OS processes.</p> <p>The <code>isolate</code> step applies a mechanism to separate idle power from absolute power, resulting in dynamic power  It also covers an implementation to separate the dynamic power consumed by background and OS processes (referred to as <code>system_processes</code>).</p> <p>It's important to note that both the idle and dynamic <code>system_processes</code> power are higher than zero, even when the metric utilization of the users' workload is zero.</p> <p>The <code>isolate</code> step applies a mechanism to separate idle power from absolute power, resulting in dynamic power. It also covers an implementation to separate the dynamic power consumed by background and OS processes (referred to as <code>system_processes</code>).</p> <p>It's important to note that both the idle and dynamic <code>system_processes</code> power are higher than zero, even when the metric utilization of the users' workload is zero.</p> <p>There are two common available <code>isolators</code>: ProfileIsolator and MinIdleIsolator.</p> <p>We refer to models trained using the isolate step as <code>DynPower</code> models. Meanwhile, models trained without the isolate step are called <code>AbsPower</code> models. Currently, the <code>DynPower</code> model does not include idle power information, but we plan to incorporate it in the future.</p> <p>On the other hand, MinIdleIsolator identifies the minimum power consumption among all samples in the training data, assuming that this minimum power consumption represents both the idle power and <code>system_processes</code> power consumption.</p> <p>While we should also remove the minimal resource utilization from the data used to train the model, this isolation mechanism includes the resource utilization by <code>system_processes</code> in the training data. However, we plan to remove it in the future.</p> <p>ProfileIsolator relies on collecting data (e.g., power and resource utilization) for a specific period without running any user workload (referred to as profile data). This isolation mechanism also eliminates the resource utilization of <code>system_processes</code> from the data used to train the model.</p> <p>On the other hand, MinIdleIsolator identifies the minimum power consumption among all samples in the training data, assuming that this minimum power consumption represents both the idle power and <code>system_processes</code> power consumption.</p> <p>While we should also remove the minimal resource utilization from the data used to train the model, this isolation mechanism includes the resource utilization by <code>system_processes</code> in the training data. However, we plan to remove it in the future.</p> <p>If the <code>profile data</code> that matches a given <code>node_type</code> exist, the pipeline will use the ProfileIsolator to pre-process the training data. Otherwise, the the pipeline will applied another isolation mechanism, such as the MinIdleIsolator.</p> <p>(check how profiles are generated here)</p>"},{"location":"kepler_model_server/pipeline/#discussion","title":"Discussion","text":"<p>The choice between using the <code>DynPower</code> or <code>AbsPower</code> model is still under investigation. In some cases, DynPower exhibits better accuracy than <code>AbsPower</code>. However, we currently utilize the <code>AbsPower</code> model to estimate node power for Platform, CPU and DRAM components, as the <code>DynPower</code> model lacks idle power information.</p> <p>It's worth mentioning that exposing idle power on a VM in a public cloud environment is not possible. This is because the host's idle power must be distributed among all running VMs on the host, and it's impossible to determine the number of VMs running on the host in a public cloud environment.</p> <p>Therefore, we can only expose idle power if there is only one VM running on the node (for a very specific scenario), or if the power model is being used in Bare Metal environments.</p>"},{"location":"kepler_model_server/pipeline/#trainer","title":"Trainer","text":"<p><code>trainer</code> is an abstraction to define the learning method applies to each feature group with each given power labeling source.</p> <p>Available trainer (v0.6):</p> <ul> <li>PolynomialRegressionTrainer</li> <li>GradientBoostingRegressorTrainer</li> <li>SGDRegressorTrainer</li> <li>KNeighborsRegressorTrainer</li> <li>LinearRegressionTrainer</li> <li>SVRRegressorTrainer</li> </ul>"},{"location":"kepler_model_server/pipeline/#node-type","title":"Node type","text":"<p>Kepler forms multiple groups of machines (nodes) based on its benchmark performance and trains a model separately for each group. The identified group is exported as <code>node type</code>.</p>"},{"location":"kepler_model_server/power_estimation/","title":"Kepler Power Estimation Deployment","text":"<p>In Kepler, we also provide a power estimation solution from the resource usages in the system that there is no power measuring tool installed or supported. There are two alternatives of estimators.</p>"},{"location":"kepler_model_server/power_estimation/#estimators","title":"Estimators","text":"<ul> <li> <p>Local Linear Regression Estimator: This estimator estimates power using the trained weights multiplied by normalized value of usage metrics (Linear Regression Model).</p> </li> <li> <p>General Estimator Sidecar: This estimator transforms the usage metrics and applies with the trained models which can be any regression models from scikit-learn library or any neuron networks from Keras (TensorFlow). To use this estimator, the Kepler estimator needs to be enabled.</p> </li> </ul> <p>On top of that, the trained models as well as weights can be updated periodically with online training routine by connecting the Kepler model server API.</p>"},{"location":"kepler_model_server/power_estimation/#deployment-scenarios","title":"Deployment Scenarios","text":""},{"location":"kepler_model_server/power_estimation/#minimum-deployment","title":"Minimum Deployment","text":"<p>The minimum deployment is to use local linear regression estimator in Kepler main container with only offline-trained model weights.</p> <p></p>"},{"location":"kepler_model_server/power_estimation/#deployment-with-general-estimator-sidecar","title":"Deployment with General Estimator Sidecar","text":"<p>To enable general estimator for power inference, the estimator sidecar can be deployed as shown in the following figure. The connection between two containers is a unix domain socket which is lightweight and fast. Unlike the local estimator, the general estimator sidecar is instrumented with several inference-supportive libraries and dependencies. This additional overhead must be tradeoff to an increasing estimation accuracy expected from flexible choices of models.</p> <p></p>"},{"location":"kepler_model_server/power_estimation/#minimum-deployment-connecting-to-kepler-model-server","title":"Minimum deployment connecting to Kepler Model Server","text":"<p>To get the updated weights which is expected to provide better estimation accuracy, Kepler may connect to remote Kepler Model Server that performs online training using data from the system with the power measuring tool as below.</p> <p></p>"},{"location":"kepler_model_server/power_estimation/#full-deployment","title":"Full deployment","text":"<p>The following figure shows the deployment that Kepler General Estimator is enabled and it is also connecting to remote Kepler Model Server. The Kepler General Estimator sidecar can update the model from the Kepler Model Server on the fly and expect the most accurate model.</p> <p></p>"},{"location":"kepler_model_server/power_estimation/#provided-power-models-on-kepler-model-db","title":"Provided power models on Kepler Model DB","text":"version power data source pipeline available energy sources error report 0.6 nx12 std_v0.6 rapl,acpi Link 0.7 SPECpower specpower acpi Link 0.7 Training Playbook ec2 intel_rapl Link"},{"location":"platform-validation/","title":"Platform Validation Framework","text":"<p>In this document, we will share the design and implementation of Kepler's platform validation framework. Please refer to enhancement document for the feature initiative and scope.</p> <p>Kepler should and will integrate with various of hardware platforms, the framework will first use Intel X86 BareMetal platform as example to show the platform validation mechanism and workflows. Other platform owners could use this document as reference to add their specific test cases and workflows to make Kepler better engage with their platforms.</p>"},{"location":"platform-validation/#mechanism-and-methodology","title":"Mechanism and methodology","text":"<p>Platform validation work should be done automatically, could follow the standard Github Action workflow mechanism and let the target platform be self-hosted runner. See Github action official document for more details about self-hosted runner.</p> <p>Platform validation cases should follow the current Kepler's Ginkgo test framework.</p> <p>We could leverage the Ginkgo Reporting Infrastructure to generate test report in both human and machine readable formats, such as JSON.</p> <p>Platform validation cases should support both validity and accuracy check on Kepler/Prometheus exposed data.</p> <p>Take Intel X86 BareMetal platform's validation as example, we have introduced an independent RAPL-based energy collection and power consumption calculation tool called <code>validator</code>.</p> <p>The current work mechanism and features of <code>validator</code> are simple:</p> <ol> <li>It could detect the current platform's CPU model type.</li> <li>It could detect the current platform's in-band RAPL components support status and OOB platform power source   support status.</li> <li>It uses specific sampling count(configurable, by default 20 sampling cycles) with specific sampling interval   (configurable, by default 15 seconds) to collect the specific components' RAPL values and calculate the power   consumption of each sampling, then achieve the mean value of node components power among those sampling cycles.</li> </ol> <p>Test cases could use above sampling and calculation results as comparison base to check the Kepler exported and Prometheus aggregated query results.</p> <p>For other platforms, developers may use other specific measurement methods and tools to implement similar validation targets and logic.</p>"},{"location":"platform-validation/#data-validity-check","title":"Data validity check","text":"<p>Intel X86 Platforms could have different CPU models which are exported by Kepler, this should be verified in independent way. The case setup and execution should have no extra dependencies on the platform OS and software packages/libraries.</p> <p>For Intel X86 CPUs, especially the new coming ones, the cpu model detail information comes with <code>cpuid</code> tool. On Linux Distros such as Ubuntu, the <code>cpuid</code> tool version is often not up-to-date, so it is better way to design an OS agnostic container to perform the test.</p> <p>For Intel X86 platforms, the specific RAPL domains available vary across product segments.</p> <p>Platforms targeting the client segment support the following RAPL domain hierarchy:</p> <ul> <li> <p>Package</p> </li> <li> <p>Two power planes: PP0 and PP1 (PP1 may reflect to uncore devices)</p> </li> </ul> <p>Platforms targeting the server segment support the following RAPL domain hierarchy:</p> <ul> <li> <p>Package</p> </li> <li> <p>Power plane: PP0</p> </li> <li> <p>DRAM</p> </li> </ul> <p>We need to check the current hardware's component power source support status and check if the exposed data is expected (zero or non-zero).</p>"},{"location":"platform-validation/#data-accuracy-check","title":"Data accuracy check","text":"<p>For Intel X86 platforms, since the RAPL based system power collection is available on BareMetal, Kepler uses Power Ratio Modeling.</p> <p>Such power attribution mechanism accuracy should be checked.</p> <p>We need to introduce an independent and platform agnostic way to collect the node components power info.</p> <p>Due to the root privilege limit on the access of the RAPL SYSFS files, we need to use privileged container to perform the test.</p> <p>For node level power accuracy check, the comparison logic is simple and straightforward, while for pod/container level power accuracy check, the logic is a little bit complicated and need some assumptions.</p> <ol> <li> <p>RAPL energy is node/package level, so we could only use RAPL sampling delta to calculate the power change introduced by application deployment and undeployment.</p> </li> <li> <p>On the other hand, application power consumption data could be queried from Prometheus where Kepler's exported time-series metrics are aggregated.</p> </li> <li> <p>On a clean/idle platform, without other tenants' workloads interference, we could have a simple assumption that the power increase introduced by the specific application deployment is equal to the test container calculated power delta in (1).</p> </li> <li> <p>On a busy platform, with other tenants' workloads running in parallel, in container or in VM, for instance, the scenarios become complicated. Since the interference power may be fluctuated and unable to detect, the Kepler <code>Power Ratio Modeling</code> check criteria is unknown, now we just dump all the available data and leave the result for evaluation after platform validation test.</p> </li> <li> <p>On some CPU/GPU-intensive workloads deployment platform, such as AI/AIGC inference pipeline's worker node, the <code>validator</code> raw data could be used to measure the approximate power consumption contribution of those inferencing pods/containers. We can then compare the Kepler demonstrated power consumption data, either in PromQL query result(automation) or on Grafana Dashboard(manual test).</p> </li> </ol> <p>This is also meaningful test case for the carbon footprint accuracy check on AI/AIGC workloads.</p>"},{"location":"platform-validation/#validation-workflow","title":"Validation workflow","text":"<p>See example Github action workflow here.</p> <p>The workflow is similar as the e2e integration test workflow, has two phases jobs:</p>"},{"location":"platform-validation/#artifacts-build","title":"Artifacts build","text":"<p>Build both the <code>Kepler</code> container image and the <code>Validator</code> container image.</p> <p>The current Validator image refers to the Kepler image build mechanism to build from a GPU image to support GPU case. We may simply build it from UBI image later.</p> <p>Since it is an independent energy collector other than Kepler, those eBPF stuffs will not be bumped into the container.</p>"},{"location":"platform-validation/#platform-validation-test","title":"Platform validation test","text":"<p>The major steps in this job are as follows:</p> <ol> <li> <p>Download artifacts and import images.</p> </li> <li> <p>Run <code>validator</code> image to collect energy and calculate node components power consumption data before deploying   local-dev-cluster(Kind in this example)</p> </li> <li> <p>Use Kepler action to deploy local-dev-cluster(Kind) with local registry support and full kube-prometheus   monitoring stack.</p> </li> <li> <p>Run <code>validator</code> image again to collect energy and calculate node components power consumption data before   deploying Kepler exporter.</p> </li> <li> <p>Deploy Kepler exporter in local-dev-cluster above and let it be monitored by Prometheus.</p> </li> <li> <p>Run <code>validator</code> image the 3rd time to collect energy and calculate node components power consumption data   after deploying Kepler exporter.</p> </li> <li> <p>Run platform-validation test scripts, do below things:</p> <ol> <li> <p><code>port forwarding</code> kepler-exporter and prometheus-k8s service to let their ports accessible to Ginkgo test code.</p> </li> <li> <p>Use <code>ginkgo</code> CLI to run test suite, execute cases described in section Mechanism and methodology above and generate test report in <code>platform_validation_report.json</code> file assigned by <code>--json-report</code> parameter.</p> </li> <li> <p>Dump necessary test log and data for post-test evaluation use.</p> </li> </ol> </li> <li> <p>Cleanup the test environment(undeploy Kepler exporter, the local-dev-cluster and the local registry).</p> </li> </ol> <p>Note</p> <p>In specific test cases, the power data delta of step 4 and 2 could be comparison base for local-dev-cluster's power consumption; while the power data delta of step 6 and 4 could be the kepler exporter's power comparison base.</p>"},{"location":"platform-validation/#validation-result-evaluation","title":"Validation result evaluation","text":"<p>Check validation report for any failure cases.</p> <p>Check accuracy cases comparison result. Node level cases should be more accurate than pod/container level cases, make analysis on any abnormal results and figure out RCAs for them.</p> <p>Manual/automation check for specific workloads on specific platforms, see example in (5) of <code>Data accuracy check</code> section above.</p> <p>Todo</p> <p>There is an analyzer under developing for abnormality detection in platform validation test, it will be integrated into the current workflow when it is ready.</p>"},{"location":"platform-validation/#further-works","title":"Further works","text":"<p>Identify issues based on the validation report on specific platforms and figure out solutions.</p> <p>Extend platform scenario from BareMetal to VM.</p> <p>Design validation cases for VM scenario and support more accurate power attribution among VM-based workloads and container/process-based workloads.</p>"},{"location":"project/adopters/","title":"Kepler Adopters","text":""},{"location":"project/adopters/#kepler-adopters","title":"Kepler Adopters","text":"<p>Organizations below all are using Kepler.</p> <p>To join this list, please follow these instructions.</p> <p></p> <p>Kepler</p>"},{"location":"project/contributing/","title":"Contributing","text":"<p>We welcome all kinds of contributions to Kepler from the community!</p> <p>For an in-depth guide on how to get started, checkout the Contributing Guide here.</p>"},{"location":"project/contributing/#kepler-adopters","title":"Kepler Adopters","text":"<p>You and your organization are using Kepler? That's awesome. We would love to hear from you! \ud83d\udc9a</p> <p>The yaml file in here contains a list of all Kepler adopters. If you want to add your organization to Kepler's list, just add an entry there and once merged you will be found under Kepler Adopters.</p>"},{"location":"project/contributing/#rendering-adopters","title":"Rendering Adopters","text":"<p>As part of adding an organization to the Kepler Adopters page, when data/adopters.yaml is updated, gomplate must be installed. The Kepler website uses it to render the Kepler Adopters page properly.</p> <p>Note</p> <p>These steps are only needed if data/adopters.yaml is updated as part of adding an organization to the Kepler Adopters page.</p> <ol> <li> <p>Install pkgx</p> <pre><code>curl -Ssf https://pkgx.sh | sh\n</code></pre> </li> <li> <p>Install gomplate</p> <pre><code>pkgx +gomplate.ca^v3.11.7\n</code></pre> </li> <li> <p>Enter the output from the previous command to update PATH. Example:</p> <pre><code>PATH=\"$HOME/.pkgx/gomplate.ca/v3.11.7/bin${PATH:+:$PATH}\"\n</code></pre> </li> <li> <p>Update adopters page using data from data/adopters.yaml</p> <pre><code>gomplate -d adopters=./data/adopters.yaml -f templates/adopters.md -o docs/project/adopters.md\n</code></pre> </li> </ol>"},{"location":"project/contributing/#adding-your-organization","title":"Adding Your Organization","text":"<p>To do so follow these steps:</p> <ol> <li>Fork the kepler-doc repository.</li> <li>Clone it locally with <code>git clone https://github.com/&lt;YOUR-GH-USERNAME&gt;/kepler-doc.git</code>.</li> <li>(Optional) Add the logo of your organization to docs/fig/logos. Good practice is for the logo to be called e.g. MY-ORG.png (=&gt; docs/fig/logos/default.svg is the Kepler logo, it is used when no organization logo is provided.)</li> <li> <p>Add an entry to the YAML file with the name of your organization, url that links to its website, and the path to the logo. Example:</p> <pre><code>    - name: Kepler\n      url: https://sustainable-computing.io/\n      logo: logos/kepler.svg\n</code></pre> </li> <li> <p>Verify the Kepler Adopters page updated properly by running the following commands (see    Install MkDocs    for more details on how to preview the documentation from a build):</p> <pre><code>mkdocs build\nmkdocs server\n</code></pre> </li> <li> <p>When happy with the changes, add the changed files using <code>git add -A</code> and then commit using    <code>git commit -s -m \"Add MY-ORG to adopters\"</code> (commit sign-off is required, see    DCO of the kepler project).</p> </li> <li>Push the commit with <code>git push origin main</code>.</li> <li>Open a Pull Request to kepler-doc</li> </ol>"},{"location":"project/resources/","title":"Resources","text":"<p>We really appreciate talks and demos about Kepler from the community. If you have made a presentation that demonstrated or referenced Kepler, please open a PR to add it to this page!</p> <p>To simplify the conference column, workshops and co-located events are represented by the main conference name.</p>"},{"location":"project/resources/#talkspresentations-that-demonstrate-kepler","title":"Talks/Presentations That Demonstrate Kepler","text":"<p>The list below contains talks that explain Kepler, specifically discuss Kepler, demonstrate its capabilities, and showcase how to gather energy metrics of various Kubernetes resources.</p> No. Title Speakers/Authors Conference Links 1 Sustainability the Container Native Way Huamin Chen (Red Hat) &amp; Chen Wang (IBM) OSS NA 2022 [slide] 2 Sustainability Research the Cloud Native Way Chen Wang (IBM) &amp; Huamin Chen (Red Hat) KubeCon NA 2022 [slide] 3 Sustainability in Computing: Energy Efficient Placements of Edge Workloads Parul Singh &amp; Kaiyi Liu (Red Hat) KubeCon NA 2022 [slide] 4 Green(ing) CI/CD: A Sustainability Journey with GitOps Niki Manoledaki (Weaveworks) GitOpsCon NA 2022 [slide] 5 Kepler: A framework to calculate the energy consumption of containerized applications Marcelo Amaral (IBM) &amp; Huamin Chen (RedHat) &amp; Tatsuhiro Chiba (IBM) &amp; Rina Nakazawa (IBM) &amp; Sunyanan Choochotkaew (IBM) &amp; Eun Kyung Lee (IBM) &amp; Tamar Eilam (IBM) IEEE CLOUD 2023 [paper] 6 Kepler Marcelo Amaral IEEE OSS 2023 [schedule] 7 Advancing Cloud Sustainability: A Versatile Framework for Container Power Model Training Sunyanan Choochotkaew (IBM) &amp; Chen Wang (IBM) &amp; Huamin Chen (RedHat) &amp; Tatsuhiro Chiba (IBM) &amp; Marcelo Amaral (IBM) &amp; Eun Kyung Lee (IBM) &amp; Tamar Eilam (IBM) IEEE MASCOTS 2023 [paper] 8 When Observability Meets Sustainability: A Real World Experience Marcelo Amaral (IBM), Hua Ye (IBM), Fan Jing Meng (IBM) OSS EU 2023 [session] 9 Greening the AI Cloud: Validating Power Models for Kubernetes Containers Marcelo Amaral OSS EU 2023 [recording] 10 Towards a Methodology and Framework for AI Sustainability Metrics Tamar Eilam, Pedro D. Bello-Maldonado, Bishwaranjan Bhattacharjee, Carlos Costa, Eun Kyung Lee, Asser Tantawi Hot Carbon 2023 [recording] 11 Kepler: Project Update and Deep Dive Marcelo Amaral &amp; Tatsuhiro Chiba KubeCon NA 2023 [recording] 12 Energy Observability Using Kepler: Revolutionizing Cloud Efficiency Sally OMalley &amp; Marcelo Amaral KubeCon NA 2023 [session] 13 kepler 101 Sam Yuan(IBM) KubeCon China 2023 [recording] 14 kepler deployment Sam Yuan KubeCon China 2023 [recording] 15 build kepler Sam Yuan KubeCon China 2023 [recording] 16 kepler model training Sam Yuan KubeCon China 2023 [recording] 17 \u901a\u8fc7Istio\u3001Kepler\u548c\u667a\u80fd\u8c03\u5ea6\u5b9e\u73b0\u4f18\u5316\u7684\u5fae\u670d\u52a1\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027 Towards Optimized Microservices Performance &amp; Sustainability via Istio, Kepler and Smart Scheduling Peng Hui Jiang &amp; Kevin Su (IBM) &amp; Yingchun Guo (Intel) KubeCon China 2023 [session] 18 \u5728\u7279\u5b9a\u5e73\u53f0\u4e0a\uff0c\u5f00\u666e\u52d2\u51c6\u786e\u5417\uff1f Is Kepler Accurate on Specific Platforms? Jie Ren &amp; Ken Lu (Intel) KubeCon China 2023 [session] 19 KBE Insider E27: Environmental Sustainability Marcelo Carneiro do Amaral, and Huamin Chen RedHat Developer Channel 2024 [video] 20 Optimizing Cloud Native Power Consumption using Kepler Marcelo Carneiro do Amaral Pure Performance Podcast 2024 [video] 21 Sustainable Computing: Measuring Application Energy Consumption in Kubernetes Environments with Kepler Marcelo Amaral &amp; Sunyanan Choochotkaew KubeCon EU 2024 [recording] 22 Empowering Efficiency: PEAKS - Orchestrating Power-Aware Kubernetes Scheduling Parul Singh &amp; Krishnasuri Narayanam KubeCon EU 2024 [recording] 23 Unlock Energy Consumption in the Cloud with eBPF Leonard Pahlke KubeCon EU 2024 [session] 24 Keynote: Building IT Green: A Journey of Platforms, Data, and Developer Empowerment at Deutsche Bahn Gualter Barbas Baptista (Deutsche Bahn) KubeCon EU 2024 [session] 25 Saving the Planet One Cluster at a Time: Operationalising Sustainability in Kubernetes Gabi Beyer &amp; Brendan Kamp KubeCon EU 2024 [session] 26 A Robust Power Model Training Framework for Cloud Native Runtime Energy Metric Exporter Sunyanan Choochotkaew, Chen Wang, Huamin Chen, Tatsuhiro Chiba, Marcelo Amaral, Eun Kyung Lee, Tamar Eilam Arxiv 2024 [paper] 27 Process-based Efficient Power Level Exporter Marcelo Amaral, Huamin Chen, Tatsuhiro Chiba, Rina Nakazawa, Sunyanan Choochotkaew, Eun Kyung Lee and Tamar Eilam IEEE CLOUD 2024 TBA 28 Best-Effort Power Model Serving for Energy Quantification of Cloud Instances Sunyanan Choochotkaew, Tatsuhiro Chiba, Marcelo Amaral, Rina Nakazawa, Scott Trent, Eun Kyung Lee, Umamaheswari Devi, Tamar Eilam and Huamin Chen IEEE MASCOTS 2024 TBA 29 Introduction to Monitoring Long-Term Energy Consumption and Carbon Emission With the SusQL Operator Scott Trent (IBM) Sustainability Week Local Meetup Tokyo 2024 [session] 30 Measuring the Power Consumption and Carbon Emissions of Your Local LLM on Kubernetes Akiho Miyamura Sustainability Week Local Meetup Tokyo 2024 [session]"},{"location":"project/resources/#talkspresentations-that-refer-to-kepler","title":"Talks/Presentations That Refer to Kepler","text":"<p>The list below contains talks that reference Kepler in discussions on energy efficiency and cloud-native environmental sustainability.</p> No. Title Speakers Conference Link 1 Panel Discussion: Moving Towards Environmentally Sustainable Operations with Cloud Native Tools Niki Manoledaki (Weaveworks), Chris Lavery (Weaveworks), Marlow Weston (Intel), William Caban (Red Hat) KubeCon NA 2022 [recording] 2 How to Get Involved in CNCF Environmental Sustainability TAG Marlow Weston (Intel) &amp; Huamin Chen (Red Hat) KubeCon NA 2022 [recording] 3 Smart Green Computing Cloud Native Operations William Caban &amp; Federico Rossi (Red Hat) KubeCon NA 2022 [recording] 4 Cloud Native Sustainability Efforts in the Community - TAG Environmental Sustainability Antonio Di Turi, Data Reply Gmbh &amp; Kristina Devochko KubeCon EU 2024 [session]"},{"location":"project/resources/#kepler-blogsarxiv","title":"Kepler Blogs/Arxiv","text":"<p>The list below contains blog posts or arxiv papers that describe Kepler</p> No. Title Authors Year Link 1 Exploring Kepler\u2019s potentials: unveiling cloud application power consumption Marcelo Amaral, Sunyanan Choochotkaew, Eun Kyung Lee, Huamin Chen, and Tamar Eilam 2023 [CNCF blog] 2 A Robust Power Model Training Framework for Cloud Native Runtime Energy Metric Exporter Sunyanan Choochotkaew, Chen Wang, Huamin Chen, Tatsuhiro Chiba, Marcelo Amaral, Eun Kyung Lee, Tamar Eilam 2024 [arxiv] 3 Idle Power Matters: Kepler Metrics for Public Cloud Energy Efficiency Sunyanan Choochotkaew, Marcelo Amaral, Huamin Chen 2024 [TAG ENV blog]"},{"location":"project/support/","title":"Support","text":""},{"location":"project/support/#the-best-ways-to-seek-support-are","title":"The best ways to seek support are","text":"<ol> <li> <p>Opening an issue in Kepler.</p> </li> <li> <p>Starting a discussion</p> </li> </ol>"},{"location":"usage/deep_dive/","title":"Kepler Deep Dive","text":""},{"location":"usage/deep_dive/#kepler-components-and-what-they-do","title":"Kepler components and what they do","text":"<p>The Kepler stack comprises Kepler and Kepler Model Server</p>"},{"location":"usage/deep_dive/#kepler","title":"Kepler","text":"<p>Kepler, Kubernetes-based Efficient Power Level Exporter, offers a way to estimate power consumption at the process, container, and Kubernetes pod levels.</p> <p>How Kepler collects data?</p> <p>Kepler uses the following to collects power data:</p>"},{"location":"usage/deep_dive/#ebpf-hardware-counters","title":"EBPF, Hardware Counters","text":"<p>Kepler can utilize a BPF program integrated into the kernel's pathway to extract process-related resource utilization metrics or use metrics from Hardware Counters. The type of metrics used to build the model can differ based on the system's environment. For example, it might use hardware counters, or metrics from tools like eBPF, depending on what is available in the system that will use the model.</p>"},{"location":"usage/deep_dive/#real-time-component-power-meters","title":"Real-time Component Power Meters","text":"<p>Kepler also collects real-time power consumption metrics from the node components using various APIs such as,</p> <ul> <li>Intel Running Average Power Limit (RAPL) for CPU and DRAM power</li> <li>NVIDIA Management Library (NVML) for GPU power</li> </ul>"},{"location":"usage/deep_dive/#platform-power-meters","title":"Platform Power Meters","text":"<p>For platform power, i.e, the entire node power Kepler uses:</p> <ul> <li>Advanced Configuration and Power Interface (ACPI)</li> <li>Redfish/Intelligent Power Management Interface (IPMI)</li> <li>Regression-based Trained Power Models when no real-time power metrics are available in the system.</li> </ul>"},{"location":"usage/deep_dive/#kepler-model-server","title":"Kepler Model Server","text":"<p>The Model Server is used to train power models, and it can be optionally deployed alongside Kepler to help Kepler select the most appropriate power model for a given environment. For example, considering the CPU model, available metrics and the required model accuracy. In the future, Kepler will also be able to select the power model with the same logic that the Model Server has.</p> <p>The Model Server trains its models using Prometheus metrics from a specific bare-metal node. It records how much energy the node consumed and the resource utilization of containers and system processes (OS and other background processes). The container metrics are obtained from running various small tests that stress different resources (CPU, memory, cache, etc.), like using a tool called <code>stress-ng</code>. When creating the power model, the Model Server uses a regression algorithm. It keeps training the model until it reaches an acceptable level of accuracy.</p> <p>Once trained, the Model Server makes these models accessible through a github repository, where any Kepler deployment can download the model from. Kepler then uses these models to calculate how much power a node (VM) consumes based on the way its resources are being used. The type of metrics used to build the model can differ based on the system's environment. For example, it might use hardware counters, or metrics from tools like eBPF, depending on what is available in the system that will use the model.</p> <p></p> <p>For details on the architecture follow the documentation on Kepler Model Server.</p>"},{"location":"usage/deep_dive/#collecting-system-power-consumption-vms-versus-bms","title":"Collecting System Power Consumption \u2013 VMs versus BMs","text":"<p>Depending on the environment that Kepler was deployed in, the system power consumption metrics collection will vary. For example, consider the figure below, Kepler can be deployed either through BMs or VMs environments.</p> <p></p>"},{"location":"usage/deep_dive/#direct-real-time-system-power-metrics-bare-metals","title":"Direct Real-Time System Power Metrics (Bare Metals)","text":"<p>In bare-metal environments that allow the direct collection of real-time system power metrics, Kepler can split the power consumption of a given system resource using the Ratio Power model. The APIs that expose the real-time power metrics export the absolute power, which is the sum of the dynamic and idle power. To be more specific, the dynamic power is directly related to the resource utilization and the idle power is the constant power that does not vary regardless if the system is at rest or with load. This concept is important because the idle and dynamic power are split differently across all processes.</p>"},{"location":"usage/deep_dive/#estimated-system-power-metrics-virtual-machines","title":"Estimated System Power Metrics (Virtual Machines)","text":"<p>In VM environments on public clouds, there is currently no direct way to measure the power that a VM consumes. Therefore, we need to estimate the power using a trained power model, which has some limitations that impact the model accuracy.</p> <p>Kepler can estimate the dynamic power consumption of VMs using trained power models. Then, after estimating each VM's power consumption, Kepler applies the Ratio Power Model to estimate the processes' power consumption. However, since VMs usually do not provide hardware counters, Kepler uses eBPF metrics instead of hardware counters to calculate the ratios. It is important to highlight that trained power models used for VMs on a public cloud cannot split the idle power of a resource because we cannot know how many other VMs are running in the host. We provide more details in the limitation section in this blog. Therefore, Kepler does not expose the idle power of a container running on top of a VM.</p> <p>Power models are trained by performing regression analysis (like Linear or Machine Learning (ML)-based regression) on data collected during benchmark tests. This data includes both resource utilization and power consumption on a Bare-metal node, forming the foundation for the power model estimation.</p>"},{"location":"usage/deep_dive/#passthrough-estimated-vm-power-metrics","title":"Passthrough Estimated VM Power Metrics","text":"<p>Kepler is first deployed in the bare-metal node (i.e. the cloud control plane), and it continuously measures the dynamic and idle power that each VM consumes using real-time power metrics from the BM. Then, Kepler exposes this power data with the VM. This information can be made available to the VM through \u201cHypervisor Hypercalls\u201d or by saving the numbers in special files that the VM can access (e.g. cGroup file mounted in the VM). Then, by using the VM power consumption, another Kepler instance within the VM can apply the Ratio Power Model to estimate the power used by processes residing in the VMs.</p> <p>Note</p> <p>The passthrough approach is still in exploratory and currently not available in Kepler.</p>"},{"location":"usage/deep_dive/#ratio-power-model-explained","title":"Ratio Power Model Explained","text":"<p>As explained earlier the dynamic power is directly related to the resource utilization and the idle power is the constant power that does not vary regardless if the system is at rest or with load. This concept is important because the idle and dynamic power are split differently across all processes. Now we can describe the Ratio Power model, which divides the dynamic power across all processes.</p> <p>The Ratio Power model calculates the ratio of a process's resource utilization to the entire system's resource utilization and then multiplying this ratio by the dynamic power consumption of a resource. This allows us to accurately estimate power usage based on actual resource utilization, ensuring that if, for instance, a program utilizes 10% of the CPU, it consumes 10% of the total CPU power.</p> <p>The idle power estimation follows the GreenHouse Gas (GHG) protocol guideline, which defines that the constant host idle power should be split among processes/containers based on their size (relative to the total size of other containers running on the host). Additionally, it's important to note that different resource utilizations are estimated differently in Kepler. We utilize hardware counters to assess resource utilization in bare-metal environments, using CPU instructions to estimate CPU utilization, collecting cache misses for memory utilization, and assessing Streaming Multiprocessor (SM) utilization for GPUs utilization.</p>"},{"location":"usage/deep_dive/#how-is-the-power-consumption-attribution-done","title":"How is the power consumption attribution done?","text":"<p>Now that we have explained how Kepler gathers data and train model and the Ratio Power Model let's dig into the power consumption attribution.</p> <p>Once all the data that is related to energy consumption and resource utilization are collected, Kepler can calculate the energy consumed by each process. This is done by dividing the power used by a given resource based on the ratio of the process and system resource utilization. We will detail this model later on in this blog. Then, with the power consumption of the processes, Kepler aggregates the power into containers and Kubernetes Pods levels. The data collected and estimated for the container are then stored by Prometheus.</p> <p>Kepler finds which container a process belongs to by using the Process ID (PID) information collected in the BPF program, and then using the container ID, we can correlate it to the pods' name. More specifically, the container ID comes from <code>/proc/PID/cgroup</code>, and Kepler uses the Kubernetes APIServer to keep an updated list of pods that are created and removed from the node. The Process IDs that do not correlate with a Kubernetes container are classified as <code>system processes</code> (including PID 0).</p> <p>In the future, processes that run VMs will be associated with VM IDs so that Kepler can also export VM metrics.</p>"},{"location":"usage/deep_dive/#pre-trained-power-model-limitations","title":"Pre-trained Power Model Limitations","text":"<p>It's important to note that pre-trained power models have their limitations when compared to power models using real-time power metrics.</p> <ul> <li> <p>System-Specific Models: Pre-trained power models are system-specific and vary based on CPU models and architectures. While not perfect, generic models can offer insights into application power consumption, aiding energy-efficient decisions.</p> </li> <li> <p>Overestimation in VM Power Consumption: Using bare-metal power models for single VMs can lead to overestimation as these models might not reflect the actual power usage when multiple VMs share a node. The power curve might show reduced power consumption when more CPUs are in use, impacting accurate estimation.</p> </li> <li> <p>Challenges with Idle Power Allocation: Dividing idle power among VMs based on their size relative to others on the host is challenging in public cloud environments where the number of concurrently running VMs on a host is not determinable. Estimating idle power for each VM accurately becomes complex due to this limitation.</p> </li> <li> <p>Dependency on Hypervisor Reporting: Pre-trained power models for VMs rely on accurate reporting of CPU register values by the hypervisor. Overprovisioning of resources in certain public cloud VMs can impact the accuracy of resource utilization metrics, affecting the reliability of these power models.</p> </li> </ul> <p>Note</p> <p>For more detailed explanation on the limitations of pre-trained power model read the blog by Kepler's maintainers.</p> <ul> <li> Explain the models. How the models are different and is there a right use case/scenario for when to apply a particular model over another?</li> <li> AbsComponentModelWeight</li> <li> AbsComponentPower</li> <li> AbsModelWeight</li> <li> AbsPower</li> <li> DynComponentModelWeight</li> <li> DynComponentPower</li> <li> XGBoost</li> </ul>"},{"location":"usage/general_config/","title":"Configuration","text":"<p>This is a list of configurable values of Kepler System. The configuration can be also applied by defining the following CR spec if Kepler Operator is installed.</p> Point of Configuration Spec Description Default Kepler CR (single item: default) Kepler DaemonSet Deployment daemon.exporter.image Kepler main image quay.io/sustainable_computing_io/kepler:latest Kepler DaemonSet Deployment daemon.exporter.port Metric exporter port 9102 Kepler DaemonSet Deployment daemon.estimator-sidecar.enabled Kepler Estimator Sidecar patch false Kepler DaemonSet Deployment daemon.estimator-sidecar.image Kepler estimator sidecar image - Kepler DaemonSet Deployment daemon.estimator-sidecar.mnt-path Mount path between main container and the sidecar for unix domain socket /tmp Kepler DaemonSet Environment (METRIC_PATH) daemon.exporter.path Path to export metrics /metrics Kepler DaemonSet Environment (MODEL_SERVER_ENABLE) model-server.enabled Kepler Model Server Pod connection false model-server.enabled Model Server Pod Pod Environment (MODEL_SERVER_PORT) model-server.port Model serving port of model server 8100 Model Server Pod Pod Environment (PROM_SERVER) model-server.prom Endpoint to Prometheus metric server <code>http://prometheus-k8s.monitoring.svc.cluster.local:9090</code> Model Server Pod Pod Environment (MODEL_PATH) model-server.model-path Path to keep models models Kepler DaemonSet Environment (MODEL_SERVER_ENDPOINT) daemon.model-server Endpoint to server container of model server <code>http://kepler-model-server.monitoring.cluster.local:[model-server.port]/model</code> Model Server Pod Deployment model-server.trainer Model online trainer patch false model-server.trainer Model Server Pod Environment (PROM_QUERY_INTERVAL) model-server.prom_interval Interval to execute training pipelines in seconds 20 Model Server Pod Environment (PROM_QUERY_STEP) model-server.prom-step Step of query data point in training pipelines in seconds 3 Model Server Pod Environment (PROM_HEADERS) model-server.prom-header For specify required header (such as authentication) - Model Server Pod Environment (PROM_SSL_DISABLE) model-server.prom-ssl Disable ssl in Prometheus connection true Model Server Pod Environment (INITIAL_MODELS_LOC) model-server.init-loc Root URL of offline models to use as initial models <code>https://raw.githubusercontent.com/sustainable-computing-io/kepler-model-server/main/tests/test_models</code> Model Server Pod Environment (INITIAL_MODEL_NAMES.[<code>MODEL_TYPE</code>]) model-server.[<code>MODEL_TYPE</code>] Name of default pipeline for each model type - CollectMetric CR (single item: default) Kepler DaemonSet Environment (COUNTER_METRICS) counter List of performance metrics to enable from counter source * (enable all available metrics from counter source) Kepler DaemonSet Environment (BPF_METRICS) bpf List of performance metrics to enable from bpf (aka. eBPF) source * (enable all available metrics from bpf source) Kepler DaemonSet Environment (GPU_METRICS) gpu List of performance metrics to enable from gpu source * (enable all available metrics from gpu source) ExportMetric CR (single item: default) Kepler DaemonSet Environment (PERF_METRICS) perf List of performance metrics to export * (enable all collected performance metrics) Kepler DaemonSet Environment (EXPORT_NODE_TOTAL_POWER) node_total_power Toggle whether to export node total power true Kepler DaemonSet Environment (EXPORT_NODE_COMPONENT_POWERS) node_component_powers Toggle whether to export node powers by components true Kepler DaemonSet Environment (EXPORT_POD_TOTAL_POWER) pod_total_power Toggle whether to export pod total power true Kepler DaemonSet Environment (EXPORT_POD_COMPONENT_POWERS) pod_component_powers Toggle whether to export pod powers by components true EstimatorConfig CR (multiple items: node-total-power, node-component-powers, pod-total-power, pod-component-powers) Kepler DaemonSet Environment (MODEL_CONFIG.[<code>MODEL_ITEM</code>]_ESTIMATOR) use-sidecar Toggle whether to use estimator sidecar for power estimation false Kepler DaemonSet Environment (MODEL_CONFIG.[<code>MODEL_ITEM</code>]_MODEL) fixed-model Specify model name (auto-selected) Kepler DaemonSet Environment (MODEL_CONFIG.[<code>MODEL_ITEM</code>]_FILTERS) filters Specify model filter conditions in string (auto-selected) Kepler DaemonSet Environment (MODEL_CONFIG.[<code>MODEL_ITEM</code>]_INIT_URL) init-url URL to initial model location - RatioConfig CR (single items: default) Kepler DaemonSet Environment (CORE_USAGE_METRIC) core_metric Specify metric for compute core (mostly cpu) component of pod power by ratio modeling cpu_instr Kepler DaemonSet Environment (DRAM_USAGE_METRIC) dram_metric Specify metric for computing dram component of pod power by ratio modeling cache_miss Kepler DaemonSet Environment (UNCORE_USAGE_METRIC) uncore_metric Specify metric for computing uncore component of pod power by ratio modeling (evenly divided) Kepler DaemonSet Environment (GENERAL_USAGE_METRIC) general_metric Specify metric for computing uncategorized component (pkg-core-uncore) of pod power by ratio modeling cpu_instr Kepler DaemonSet Environment (GPU_USAGE_METRIC) core_metric Specify metric for computing gpu component of pod power by ratio modeling - <p>Remarks:</p> <ul> <li>[<code>MODEL_ITEM</code>] can be either of the following values corresponding to item names: <code>NODE_TOTAL</code>, <code>NODE_COMPONENT</code>, <code>POD_TOTAL</code>, <code>POD_COMPONENTS</code>.</li> <li>[<code>MODEL_TYPE</code>] is a concatenation of [<code>MODEL_ITEM</code>] and [<code>OUTPUT_FORMAT</code>] which can be either <code>POWER</code> for archived model or <code>MODEL_WEIGHT</code> for weights in json.</li> </ul> <p>For example:</p> <ul> <li><code>NODE_TOTAL_POWER</code>: archived model to estimate node total power used by estimator sidecar</li> <li><code>POD_COMPONENTS_MODEL_WEIGHT</code>: model weight to estimate pod component powers used by linear regressor embedded in Kepler main component.</li> </ul>"},{"location":"usage/kepler_daemon/","title":"Kepler DaemonSet Customization","text":"<p>Kepler enables a function to hybrid read environment variable from attributes directly (container.env) and from the ConfigMap. Note that, all steps will be operated by Kepler Operator if the operator is installed.</p> <p>To set environments by ConfigMap:</p> <ol> <li> <p>Create/Generate ConfigMap</p> <pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: kepler-cfm\n  namespace: kepler-system\ndata:\n  MODEL_SERVER_ENABLE: true\n  COUNTER_METRICS: '*'\n  BPF_METRICS: '*'\n  # KUBELET_METRICS: ''\n  # GPU_METRICS: ''\n  PERF_METRICS: '*'\n  MODEL_CONFIG: |\n    POD_COMPONENT_ESTIMATOR=true\n    POD_COMPONENT_INIT_URL=https://raw.githubusercontent.com/sustainable-computing-io/kepler-model-server/main/tests/test_models/DynComponentPower/CgroupOnly/ScikitMixed.zip\n</code></pre> </li> <li> <p>Mount the ConfigMap to DaemonSet:</p> <pre><code>  spec:\n    containers:\n      - name: kepler-exporter\n        volumeMounts:\n        - name: cfm\n          mountPath: /etc/config\n          readOnly: true\n      volumes:\n      - name: cfm\n        configMap:\n          name: kepler-cfm\n</code></pre> </li> </ol>"},{"location":"usage/trouble_shooting/","title":"Trouble Shooting","text":""},{"location":"usage/trouble_shooting/#kepler-pod-failed-to-start","title":"Kepler Pod failed to start","text":""},{"location":"usage/trouble_shooting/#background","title":"Background","text":"<p>Kepler uses eBPF to obtain performance counter readings and processes stats. Since eBPF requires kernel headers, Kepler will fail to start up when the kernel headers are missing.</p>"},{"location":"usage/trouble_shooting/#diagnose","title":"Diagnose","text":"<p>To confirm, check the Kepler Pod logs with the following command and look for message <code>not able to load eBPF modules</code>.</p> <pre><code>kubectl logs -n kepler daemonset/kepler-exporter\n</code></pre>"},{"location":"usage/trouble_shooting/#solution","title":"Solution","text":"<p>Installing kernel headers on each node can be done manually using the following command</p> <pre><code># Fedora/RHEL based distro\ndnf install kernel-devel-`uname -r` -y\n# Debian/Ubuntu distro\napt install linux-headers-$(uname -r)\n</code></pre> <p>On OpenShift, install the MachineConfiguration here</p>"},{"location":"zh/","title":"Kubernetes Efficient Power Level Exporter (Kepler)","text":"<p>Kepler (Kubernetes-based Efficient Power Level Exporter)\u662f\u4e00\u4e2aprometheus exportor\u3002 \u5b83\u901a\u8fc7eBPF\u6280\u672f\u4e0eCPU\u6027\u80fd\u8ba1\u6570\u5668\u4ee5\u53caLinux\u5185\u6838\u7684tracepoints\u8fdb\u884c\u4ea4\u4e92\u4ee5\u83b7\u5f97\u7279\u5b9a\u6570\u636e\u3002</p> <p>\u8fd9\u4e9b\u6570\u636e\u548c\u6765\u81eacgroup\u6216\u8005sysfs\u7684\u4fe1\u606f\u4e00\u8d77\u53ef\u4ee5\u653e\u5165\u673a\u5668\u5b66\u4e60\u6a21\u578b\u4e2d\u6765\u5bf9pod\u6216\u8fdb\u7a0b\u7684\u80fd\u8017\u8fdb\u884c\u9884\u6d4b\u3002</p> <p>\u9879\u76ee\u7684Github\u5730\u5740 \u27a1\ufe0f Kepler. \u76ee\u524d\u4e2d\u6587\u6587\u6863\u4f9d\u65e7\u5728\u65bd\u5de5\u4e2d\uff0c\u6b22\u8fce\u8d21\u732e\u3002</p> <p></p> <p> \u76ee\u524d\u8be5\u9879\u76ee\u5df2\u7ecf\u6210\u4e3aCloud Native Computing Foundation sandbox project.   </p>"},{"location":"zh/design/architecture/","title":"\u7ec4\u4ef6","text":""},{"location":"zh/design/architecture/#kepler-exporter","title":"Kepler Exporter","text":"<p>Kepler Exporter\u516c\u5f00\u4e86\u6709\u5173Kubernetes\u7ec4\u4ef6\uff08\u5982Pods\u548cNodes\uff09\u80fd\u8017\u7684\u5404\u79cd\u6307\u6807\u3002</p> <p>\u8bf7\u70b9\u51fb\u94fe\u63a5\u67e5\u770b\u76f8\u5173\u80fd\u8017\u6307\u6807/metrics\u7684\u5b9a\u4e49\u3002</p> <p></p>"},{"location":"zh/design/architecture/#kepler-model-server","title":"Kepler Model Server","text":"<p>Kepler Model Server\u4e3b\u8981\u63d0\u4f9b\u80fd\u8017\u9884\u4f30\u6a21\u578b\uff0c\u8be5\u6a21\u578b\u652f\u6301\u5bf9\u5404\u79cd\u7c92\u5ea6\u7684\uff08\u5982\u8282\u70b9\u6570\uff0c\u8282\u70b9CPU\u6570\uff0cPod\u6570\uff0cPod\u8fdb\u7a0b\u6570\uff09\u7684\u8bf7\u6c42\uff0c\u5e76\u8fd4\u56de\u6307\u6807\uff0c\u7cbe\u51c6\u6027\u6a21\u578b\u8fc7\u6ee4\u5668\u3002</p> <p>\u53e6\u5916\uff0c\u5728online\u8bad\u7ec3\u6a21\u5f0f\uff0cKepler Model Server\u53ef\u4ee5\u4f5c\u4e3a\u8fb9\u8f66\u90e8\u7f72\u3002\u5e76\u4e14\u6267\u884c\u8bad\u7ec3\u76f8\u5173\u6d41\u7a0b\uff0c\u5b9e\u65f6\u66f4\u65b0\u6a21\u578b\u3002</p> <p>Kepler Estimator\u4f5c\u4e3aKepler Exporter\u7684\u8fb9\u8f66\u90e8\u7f72\uff0c\u5c06\u4f1a\u4ee5\u5ba2\u6237\u7aef\u6a21\u5f0f\u8fd0\u884c\uff0c\u8bbf\u95eekepler model server\uff0c\u8bf7\u6c42\u6a21\u578b\u3002</p> <p>Kepler estimator\u4e0eKepler Exporter\u901a\u8fc7socket\u7684\u65b9\u5f0f\uff08<code>/tmp/estimator.sock</code>\uff09\u8fdb\u884c\u8fde\u63a5\u3002\u76f8\u5173\u7684\u4ee3\u7801\u5728Kepler Exporter\u7684<code>estimator.go</code>\u6587\u4ef6\u4e2d\u6709\u5b9a\u4e49\u548c\u63cf\u8ff0\u3002</p> <p>\u9879\u76ee\u5730\u5740 \u27a1\ufe0f Kepler Model Server</p>"},{"location":"zh/design/ebpf_in_kepler/","title":"Kepler\u4e2d\u7684ebpf","text":""},{"location":"zh/design/ebpf_in_kepler/#_1","title":"\u80cc\u666f","text":""},{"location":"zh/design/ebpf_in_kepler/#ebpf","title":"\u4ec0\u4e48\u662febpf?","text":"<p>eBPF\u662f\u4e00\u9879\u9769\u547d\u6027\u7684\u6280\u672f\uff0c\u8d77\u6e90\u4e8eLinux\u5185\u6838\uff0c\u53ef\u4ee5\u5728\u64cd\u4f5c\u7cfb\u7edf\u5185\u6838\u7b49\u7279\u6743\u4e0a\u4e0b\u6587\u4e2d\u8fd0\u884c\u6c99\u76d2\u7a0b\u5e8f\u3002\u5b83\u7528\u4e8e\u5b89\u5168\u6709\u6548\u5730\u6269\u5c55\u5185\u6838\u7684\u529f\u80fd\uff0c\u800c\u65e0\u9700\u66f4\u6539\u5185\u6838\u6e90\u4ee3\u7801\u6216\u52a0\u8f7d\u5185\u6838\u6a21\u5757\u3002[1]</p>"},{"location":"zh/design/ebpf_in_kepler/#kprobe","title":"\u4ec0\u4e48\u662fkprobe?","text":"<p>KProbes\u662fLinux\u5185\u6838\u7684\u4e00\u79cd\u8c03\u8bd5\u673a\u5236\uff0c\u4e5f\u53ef\u7528\u4e8e\u76d1\u89c6\u751f\u4ea7\u7cfb\u7edf\u5185\u7684\u4e8b\u4ef6\u3002KProbes\u4f7f\u60a8\u80fd\u591f\u52a8\u6001\u5730\u95ef\u5165\u4efb\u4f55\u5185\u6838\u4f8b\u7a0b\uff0c\u5e76\u4ee5\u65e0\u4e2d\u65ad\u7684\u65b9\u5f0f\u6536\u96c6\u8c03\u8bd5\u548c\u6027\u80fd\u4fe1\u606f\u3002\u60a8\u53ef\u4ee5\u5728\u51e0\u4e4e\u4efb\u4f55\u5185\u6838\u4ee3\u7801\u5730\u5740\u8bbe\u7f6e\u9677\u9631\uff0c\u6307\u5b9a\u5728\u9047\u5230\u65ad\u70b9\u65f6\u8981\u8c03\u7528\u7684\u5904\u7406\u7a0b\u5e8f\u4f8b\u7a0b\u3002[2]</p>"},{"location":"zh/design/ebpf_in_kepler/#kprobes","title":"\u5982\u4f55\u67e5\u770b\u5df2\u7ecf\u6ce8\u518c\u7684kprobes?","text":"<pre><code>sudo cat /sys/kernel/debug/kprobes/list\n</code></pre>"},{"location":"zh/design/ebpf_in_kepler/#cpu","title":"CPU\u786c\u4ef6\u4e8b\u4ef6\u76d1\u63a7","text":"<p>\u6027\u80fd\u8ba1\u6570\u5668\u662f\u5728\u5982\u4eca\u5927\u591a\u6570CPU\u4e0a\u5747\u5df2\u5b9e\u73b0\u7684\u4e00\u79cd\u7279\u6b8a\u7684\u786c\u4ef6\u8ba1\u6570\u5668\u3002\u8fd9\u4e9b\u8ba1\u6570\u5668\u5728\u7edf\u8ba1\u67d0\u4e9b\u7279\u6b8a\u7c7b\u578b\u7684\u786c\u4ef6\u4e8b\u4ef6\uff1a \u4f8b\u5982\u6267\u884c\u547d\u4ee4\uff0c\u7f13\u5b58\u5931\u6548\uff0c\u6216\u5206\u652f\u9884\u6d4b\u9519\u8bef\u7684\u540c\u65f6\u5e76\u4e0d\u4f1a\u964d\u4f4e\u5185\u6838\u6216\u8005\u7a0b\u5e8f\u6267\u884c\u901f\u5ea6\u3002[4]</p> <p>\u4f7f\u7528\u7cfb\u7edf\u8c03\u7528 <code>perf_event_open</code> [5], Linux\u7cfb\u7edf\u5141\u8bb8\u8bbe\u7f6e\u786c\u4ef6\u548c\u8f6f\u4ef6\u6027\u80fd\u7684\u6027\u80fd\u76d1\u89c6\u3002\u5b83\u8fd4\u56de\u4e00\u4e2a\u6587\u4ef6\u63cf\u8ff0\u7b26\u6765\u8bfb\u53d6\u6027\u80fd\u4fe1\u606f\u3002 \u8fd9\u4e2a\u7cfb\u7edf\u8c03\u7528\u4f7f\u7528 <code>pid</code> \u548c <code>cpuid</code> \u4f5c\u4e3a\u53c2\u6570. Kepler\u4f7f\u7528<code>pid == -1</code>\u548c<code>cpuid</code>\u4f5c\u4e3a\u5b9e\u9645\u7684cpuid\u3002 \u8fd9\u79cdpid\u548ccpu\u7684\u7ec4\u5408\u5141\u8bb8\u6d4b\u91cf\u6307\u5b9acpu\u4e0a\u7684\u6240\u6709\u8fdb\u7a0b/\u7ebf\u7a0b\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#linuxperf_event_open","title":"\u5982\u4f55\u68c0\u67e5Linux\u5185\u6838\u662f\u5426\u652f\u6301<code>perf_event_open</code>?","text":"<p>\u68c0\u67e5\u662f\u5426\u5b58\u5728<code>/proc/sys/kernel/perf_event_paranoid</code>\uff0c\u4ee5\u4e86\u89e3\u5185\u6838\u662f\u5426\u652f\u6301<code>perf_event_open</code>\u4ee5\u53ca\u5141\u8bb8\u6d4b\u91cf\u7684\u5185\u5bb9</p> <pre><code>   The perf_event_paranoid file can be set to restrict\n   access to the performance counters.\n\n   2      allow only user-space measurements (default since Linux 4.6).\n   1      allow both kernel and user measurements (default before Linux 4.6).\n   0      allow access to CPU-specific data but not raw tracepoint samples.\n  -1      no restrictions.\n\n\n   Measuring all process/threads required CAP_SYS_ADMIN capability or a value less than 1 in above file\n</code></pre> <p>CAP_SYS_ADMIN \u662f\u6700\u9ad8\u7ea7\u522b\u7684\u80fd\u529b\uff0c\u5b83\u53ef\u80fd\u5177\u6709\u4e00\u4e9b\u5b89\u5168\u5f71\u54cd\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#kepler","title":"kepler\u63a2\u6d4b\u5185\u6838\u8fdb\u7a0b","text":"<p>kepler\u6355\u6349\u5185\u6838\u51fd\u6570<code>finish_task_switch</code>[3], \u8be5\u51fd\u6570\u8d1f\u8d23\u5728\u4efb\u52a1\u5207\u6362\u53d1\u751f\u540e\u8fdb\u884c\u6e05\u7406\u3002\u7531\u4e8e\u63a2\u6d4b\u901a\u8fc7<code>kprobe</code>\u53d1\u751f\uff0c\u5bf9\u5b83\u7684\u8c03\u7528\u53d1\u751f\u5728\u8c03\u7528<code>finish_task_switch</code>\u4e4b\u524d\uff0c\u800c\u4e0d\u662f\u5728\u63a2\u6d4b\u51fd\u6570\u8fd4\u56de\u540e\u8c03\u7528<code>kretprobe</code>\u3002</p> <p>\u5f53\u5185\u6838\u53d1\u751f\u4e0a\u4e0b\u6587\u5207\u6362\u65f6\uff0c\u51fd\u6570<code>finish_task_switch</code>\u5728\u65b0\u8fdb\u7a0b\u8fdb\u5165CPU\u65f6\u88ab\u8c03\u7528\u3002\u8fd9\u4e2a\u51fd\u6570\u63a5\u53d7\u53c2\u6570\u7c7b\u578b<code>task_struct*</code>\uff0c\u8be5\u53c2\u6570\u7c7b\u578b\u5305\u542b\u6240\u6709\u5173\u4e8e\u79bb\u5f00CPU\u8fdb\u7a0b\u7684\u6240\u6709\u4fe1\u606f\u3002[3]</p> <p>kepler\u7684\u63a2\u6d4b\u51fd\u6570</p> <pre><code>int kprobe__finish_task_switch(struct pt_regs *ctx, struct task_struct *prev)\n</code></pre> <p>\u7b2c\u4e00\u4e2a\u53c2\u6570\u7684\u7c7b\u578b\u662f\u6307\u5411<code>pt_regs</code>\u7ed3\u6784\u7684\u6307\u9488\uff0c\u8be5\u7ed3\u6784\u6307\u7684\u662f\u5728\u5185\u6838\u51fd\u6570\u6761\u76ee\u65f6\u4fdd\u6301CPU\u5bc4\u5b58\u5668\u72b6\u6001\u7684\u7ed3\u6784\u3002\u6b64\u7ed3\u6784\u5305\u542b\u4e0eCPU\u5bc4\u5b58\u5668\u76f8\u5bf9\u5e94\u7684\u5b57\u6bb5\uff0c\u4f8b\u5982\u901a\u7528\u5bc4\u5b58\u5668\uff08\u4f8b\u5982\uff0cr0\u3001r1\u7b49\uff09\u3001\u5806\u6808\u6307\u9488\uff08sp\uff09\u3001\u7a0b\u5e8f\u8ba1\u6570\u5668\uff08pc\uff09\u548c\u5176\u4ed6\u7279\u5b9a\u4e8e\u4f53\u7cfb\u7ed3\u6784\u7684\u5bc4\u5b58\u5668\u3002 \u7b2c\u4e8c\u4e2a\u53c2\u6570\u662f\u6307\u5411<code>task_struct</code>\u7684\u6307\u9488\uff0c\u8be5\u6307\u9488\u5305\u542b\u524d\u4e00\u4efb\u52a1\u7684\u4efb\u52a1\u4fe1\u606f\uff0c\u5373\u79bb\u5f00CPU\u7684\u4efb\u52a1\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#keplercpu","title":"Kepler\u76d1\u63a7CPU\u786c\u4ef6\u4e8b\u4ef6","text":"<p>Kepler\u76d1\u63a7\u4ee5\u4e0bCPU\u786c\u4ef6\u4e8b\u4ef6</p> PERF Type Perf Count Type Description Array name (in bpf program) PERF_TYPE_HARDWARE PERF_COUNT_HW_CPU_CYCLES Total CPU cycles; can get affected by CPU frequency scaling cpu_cycles_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_REF_CPU_CYCLES Total CPU cycles; not affected by CPU frequency scaling cpu_ref_cycles_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_INSTRUCTIONS Retired instructions.  Be careful, these can be affected by various issues, most notably hardware interrupt counts. cpu_instr_hc_reader PERF_TYPE_HARDWARE PERF_COUNT_HW_CACHE_MISSES Cache misses. Usually this indicates Last Level Cache misses; this is intended to be used in conjunction with the PERF_COUNT_HW_CACHE_REFERENCES event to calculate cache miss rates. cache_miss_hc_reader <p>\u6027\u80fd\u8ba1\u6570\u5668\u901a\u8fc7\u7279\u6b8a\u7684\u6587\u4ef6\u63cf\u8ff0\u7b26\u8fdb\u884c\u8bbf\u95ee\u3002\u6bcf\u4e2a\u4f7f\u7528\u7684\u865a\u62df\u8ba1\u6570\u5668\u90fd\u6709\u4e00\u4e2a\u6587\u4ef6\u63cf\u8ff0\u7b26\u3002\u6587\u4ef6\u63cf\u8ff0\u7b26\u4e0e\u76f8\u5e94\u7684\u6570\u7ec4\u76f8\u5173\u8054\u3002\u5f53\u4f7f\u7528bcc\u5305\u88c5\u5668\u51fd\u6570\u65f6\uff0c\u5b83\u8bfb\u53d6\u76f8\u5e94\u7684fd\u5e76\u8fd4\u56de\u503c\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#cpu_1","title":"\u8ba1\u7b97\u8fdb\u7a0bCPU\u8fd0\u884c\u603b\u65f6\u95f4","text":"<p>ebpf\u51fd\u6570(<code>bpfassets/perf_event/perf_event.c</code>)\u7ef4\u62a4\u4e00\u4e2a\u57fa\u4e8e\u65f6\u95f4\u6233\u5bf9\u4e8e<code>&lt;pid, cpuid&gt;</code>\u8868\u3002\u65f6\u95f4\u6233\u8868\u793a\u5728cpu\u4e0a\u8c03\u5ea6pid\u65f6\u4e3apid\u8c03\u7528<code>kprobe_finish_task_switch</code>\u7684\u65f6\u523b<code>&lt;cpuid&gt;</code></p> <p><pre><code>// &lt;Task PID, CPUID&gt; =&gt; Context Switch Start time\n\ntypedef struct pid_time_t { u32 pid; u32 cpu; } pid_time_t; \nBPF_HASH(pid_time, pid_time_t); \n// pid_time is the name of variable which if of type map\n</code></pre> \u5728\u51fd\u6570<code>get_on_cpu_time</code>\u4e2d\uff0c\u5f53\u524d\u65f6\u95f4\u6233\u4e0e<code>pid_time</code>\u6620\u5c04\u4e2d\u7684\u65f6\u95f4\u6233\u4e4b\u95f4\u7684\u5dee\u7528\u4e8e\u8ba1\u7b97\u5f53\u524dcpu\u4e0a\u5148\u524d\u4efb\u52a1\u7684<code>on_cpu_time_delta</code>\u3002</p> <p>\u6b64<code>on_cpu_time_delta</code>\u7528\u4e8e\u7d2f\u79ef\u524d\u4e00\u4efb\u52a1\u7684<code>process_run_time</code>\u5ea6\u91cf\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#cpu_2","title":"\u8ba1\u7b97\u8fdb\u7a0bCPU\u5468\u671f","text":"<p>\u5bf9\u4e8e\u8fdb\u7a0b\u7684CPU\u5468\u671f\uff0cbpf\u7a0b\u5e8f\u7ef4\u62a4<code>cpu_cycles</code>\u6570\u7ec4\uff0c\u5e76\u901a\u8fc7<code>cpuid</code>\u4f5c\u4e3a\u7d22\u5f15\u3002\u8fd9\u4e2a\u6570\u7ec4\u5305\u542b\u6027\u80fd\u6570\u7ec4<code>cpu_cycles_hc_reader</code>\uff0c\u662f\u4e00\u4e2a\u6027\u80fd\u4e8b\u4ef6\u7684\u6570\u7ec4\u3002</p> <p>\u5728\u6bcf\u4e2a\u4efb\u52a1\u5207\u6362\u4e0a\uff0c - \u4ece\u6027\u80fd\u8ba1\u6570\u5668\u9635\u5217cpu_cycles_hc_reader\u8bfb\u53d6\u5f53\u524d\u503c - \u68c0\u7d22cpucycles\u4e2d\u7684\u4e0a\u4e00\u4e2a\u503c - delta\u662f\u901a\u8fc7\u4ece\u5f53\u524d\u503c\u51cf\u53bb\u4e4b\u524d\u7684\u503c\u6765\u8ba1\u7b97\u7684 - \u5f53\u524d\u503c\u88ab\u590d\u5236\u56decpucycles\uff0c\u7528\u4e8e\u4e0b\u4e00\u4e2a\u4efb\u52a1\u5207\u6362</p> <p>\u7531\u6b64\u8ba1\u7b97\u51fa\u7684\u589e\u91cf\u662f\u79bb\u5f00cpu\u7684\u8fdb\u7a0b\u6240\u4f7f\u7528\u7684cpu\u5468\u671f\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#cpu_3","title":"\u8ba1\u7b97\u8fdb\u7a0b\u53c2\u8003CPU\u5468\u671f","text":"<p>\u4e0e\u8ba1\u7b97CPU\u5468\u671f\u7684\u8fc7\u7a0b\u76f8\u540c\uff0c\u6240\u4f7f\u7528\u7684\u6027\u80fd\u6570\u7ec4\u7684\u66ff\u6362\u4e3a<code>cpu_ref_cycles_hc_reader</code>\uff0cprev\u503c\u5b58\u50a8\u5728<code>CPU_ref_cycles</code>\u4e2d</p>"},{"location":"zh/design/ebpf_in_kepler/#cpu_4","title":"\u8ba1\u7b97\u8fdb\u7a0bCPU\u6307\u4ee4","text":"<p>\u4e0e\u8ba1\u7b97CPU\u5468\u671f\u7684\u8fc7\u7a0b\u76f8\u540c\uff0c\u6240\u4f7f\u7528\u7684\u6027\u80fd\u6570\u7ec4\u7684\u66ff\u6362\u4e3a<code>cpu_instr_hc_reader</code>\uff0cprev\u503c\u5b58\u50a8\u5728<code>cpu_instr</code>\u4e2d</p>"},{"location":"zh/design/ebpf_in_kepler/#_2","title":"\u8ba1\u7b97\u8fdb\u7a0b\u7f13\u5b58\u5931\u6548","text":"<p>\u4e0e\u8ba1\u7b97CPU\u5468\u671f\u7684\u8fc7\u7a0b\u76f8\u540c\uff0c\u6240\u4f7f\u7528\u7684\u6027\u80fd\u6570\u7ec4\u7684\u66ff\u6362\u4e3a<code>cache_miss_hc_reader</code>\uff0cprev\u503c\u5b58\u50a8\u5728<code>cache_miss</code>\u4e2d</p>"},{"location":"zh/design/ebpf_in_kepler/#cpu_5","title":"\u8ba1\u7b97CPU\u4e0a\u5e73\u5747\u9891\u7387","text":"<pre><code>avg_freq = ((on_cpu_cycles_delta * CPU_REF_FREQ) / on_cpu_ref_cycles_delta) * HZ;\n\nCPU_REF_FREQ = 2500 \nHZ = 1000\n</code></pre> <p>\u6b64\u503c\u5b58\u50a8\u5728\u6570\u7ec4<code>cpu_freq_array</code>\u4e2d</p>"},{"location":"zh/design/ebpf_in_kepler/#_3","title":"\u8fdb\u7a0b\u8868","text":"<p>bpf\u7a0b\u5e8f\u7ef4\u62a4\u4e00\u4e2a\u540d\u4e3a<code>processes</code>\u7684bpf\u6563\u5217\u3002\u6b64\u6563\u5217\u7ef4\u62a4\u4e3a\u8fdb\u7a0b\u8ba1\u7b97\u7684\u6570\u636e\u3002Kepler\u4ece\u8fd9\u4e2a\u6563\u5217\u4e2d\u8bfb\u53d6\u503c\uff08\u5728bcc\u4e2d\u79f0\u4e3a<code>Table</code>\uff09\u5e76\u751f\u6210\u5ea6\u91cf\u3002</p> Key Value Description pid cgroupid Process CGroupID pid Process ID process_run_time Total time a process occupies CPU (calculated each time process leaves CPU on context switch) cpu_cycles Total CPU cycles consumed by process cpu_instr Total CPU instructions consumed by process cache_miss Total Cache miss by process vec_nr Total number of soft irq handles by process (max 10) comm Process name (max length 16) <p>\u6b64\u6563\u5217\u7531<code>container_hc_collector.go</code>\u4e2d\u7684\u5185\u6838\u6536\u96c6\u5668\u8bfb\u53d6\uff0c\u7528\u4e8e\u6536\u96c6\u6307\u6807\u3002</p>"},{"location":"zh/design/ebpf_in_kepler/#_4","title":"\u53c2\u8003","text":"<p>[1] https://ebpf.io/what-is-ebpf/ , https://www.splunk.com/en_us/blog/learn/what-is-ebpf.html , https://www.tigera.io/learn/guides/ebpf/</p> <p>[2] An introduction to KProbes , Kernel Probes (Kprobes)</p> <p>[3] finish_task_switch - clean up after a task-switch</p> <p>[4] Performance Counters for Linux</p> <p>[5] perf_event_open(2) \u2014 Linux manual page</p>"},{"location":"zh/design/power_model/","title":"Kepler\u529f\u7387\u6a21\u578b/\u8fd0\u884c\u65b9\u5f0f","text":"<p>\u5728Kepler\u4e2d\uff0c\u6839\u636e\u53ef\u7528\u7684\u6d4b\u91cf\u7ed3\u679c\uff0c\u6211\u4eec\u901a\u8fc7\u4e24\u79cd\u529f\u7387\u5efa\u6a21\u65b9\u6cd5\u7684\u6df7\u5408\u63d0\u4f9b\u4e86\u63d0\u4f9bPod\u7ea7\u522b\u529f\u7387\uff1a</p>"},{"location":"zh/design/power_model/#_1","title":"\u5efa\u6a21\u65b9\u6cd5","text":"<p>-\u529f\u7387\u6bd4\u5efa\u6a21\uff1a\u8be5\u5efa\u6a21\u901a\u8fc7\u529f\u7387\u603b\u548c\u7684\u4f7f\u7528\u7387\u6765\u8ba1\u7b97\u66f4\u7ec6\u7c92\u5ea6\u7684\u529f\u7387\u3002\u5f53\u603b\u529f\u7387\u5df2\u77e5\u65f6\uff0c\u9ed8\u8ba4\u60c5\u51b5\u4e0b\u4f1a\u4f7f\u7528\u6b64\u5efa\u6a21\u3002</p> <p>-\u529f\u7387\u4f30\u8ba1\u5efa\u6a21\uff1a\u8be5\u5efa\u6a21\u901a\u8fc7\u4f7f\u7528\u5ea6\u91cf\u4f5c\u4e3a\u8bad\u7ec3\u6a21\u578b\u7684\u8f93\u5165\u7279\u5f81\u6765\u4f30\u8ba1\u529f\u7387\u3002\u5373\u4f7f\u4e0d\u80fd\u6d4b\u91cf\u529f\u7387\u5ea6\u91cf\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528\u6b64\u5efa\u6a21\u3002\u4f30\u8ba1\u53ef\u4ee5\u5206\u4e3a\u4e09\u4e2a\u7ea7\u522b\uff1a\u8282\u70b9\u603b\u529f\u7387\uff08\u5305\u62ec\u98ce\u6247\u3001\u7535\u6e90\u7b49\uff09\u3001\u8282\u70b9\u5185\u90e8\u7ec4\u4ef6\u529f\u7387\uff08\u5982CPU\u3001\u5185\u5b58\uff09\u3001Pod\u529f\u7387\u3002\u53e6\u8bf7\u53c2\u9605\u5f00\u59cb\u4f7f\u7528Kepler\u6a21\u578b\u670d\u52a1\u5668</p> <p>-\u9884\u8bad\u7ec3\u529f\u7387\u6a21\u578b\uff1a\u6211\u4eec\u4e3a\u4e0d\u540c\u7684\u90e8\u7f72\u573a\u666f\u63d0\u4f9b\u9884\u8bad\u7ec3\u7684\u529f\u7387\u6a21\u578b\u3002\u5f53\u524d\u7684x86_64\u9884\u8bad\u7ec3\u6a21\u578b\u662f\u5728Intel\u00ae Xeon\u00ae Processor E5-2667 v3\u4e2d\u5f00\u53d1\u7684\u3002\u5176\u4ed6\u67b6\u6784\u7684\u6a21\u578b\u5373\u5c06\u63a8\u51fa\u3002\u60a8\u53ef\u4ee5\u5728Kepler Model DB\u4e2d\u627e\u5230\u8fd9\u4e9b\u6a21\u578b\u3002\u8fd9\u4e9b\u6a21\u578b\u652f\u6301RAPL\u548cACPI\u7535\u6e90\u7684\u529f\u7387\u6bd4\u4f8b\u5efa\u6a21\u548c\u529f\u7387\u4f30\u7b97\u5efa\u6a21\u3002AbsPower\u6a21\u578b\u4f30\u7b97\u9759\u6001\u548c\u52a8\u6001\u529f\u7387\uff0c\u800cDynPower\u6a21\u578b\u53ea\u4f30\u7b97\u52a8\u6001\u529f\u7387\u3002\u8fd9\u4e9b\u6a21\u578b\u7684MAE\uff08\u5e73\u5747\u7edd\u5bf9\u8bef\u5dee\uff09\u4e5f\u5df2\u53d1\u5e03\u3002Kepler\u5bb9\u5668\u955c\u50cf\u5df2\u9884\u52a0\u8f7dacpi/AbsPower/BPFOnly/SGDRegressorTrainer_1.json\u6a21\u578b\u7528\u4e8e\u8282\u70b9\u80fd\u91cf\u4f30\u7b97\uff0c\u4ee5\u53carapl/AbsPower/BPFOnly/SGDRegressorTrainer_1.json\u7528\u4e8e\u5bb9\u5668\u7edd\u5bf9\u529f\u7387\u4f30\u7b97\u3002</p>"},{"location":"zh/design/power_model/#_2","title":"\u4f7f\u7528\u573a\u666f","text":"Scenario Node Total Power Node Component Powers Pod Power BM (x86 with power meter) Measurement (e.g., ACPI) Measurement (RAPL) Power Ratio BM (x86 but no power meter) Power Estimation Measurement Power Ratio BM (non-x86 with power meter) Measurement Power Estimation Power Ratio BM (non-x86 and no power meter) Power Estimation Power Estimation Power Ratio VM with node info and power passthrough from BM (x86 with power meter) Measurement + VM Mapping Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (x86 but no power meter) Power Estimation Measurement + VM Mapping Power Ratio VM with node info and power passthrough from BM (non-x86 with power meter) Measurement + VM Mapping Power Estimation Power Ratio VM with node info Power Estimation Power Estimation Power Ratio Pure VM - - Power Estimation"},{"location":"zh/installation/community-operator/","title":"Kepler \u793e\u533a Operator on OpenShift","text":""},{"location":"zh/installation/community-operator/#_1","title":"\u9700\u6c42","text":"<p>\u8bf7\u786e\u5b9a\u60a8\u62e5\u6709:</p> <ul> <li>\u4e00\u4e2aOCP 4.13\u96c6\u7fa4</li> <li>\u6709<code>kubeadmin</code> \u6216\u8005 <code>cluster-admin</code> \u6743\u9650\u7684\u7528\u6237\u3002</li> <li><code>oc</code> \u547d\u4ee4.</li> <li>\u4e0b\u8f7d\u4e86<code>kepler-operator</code>repository. <pre><code>git clone https://github.com/sustainable-computing-io/kepler-operator.git\ncd kepler-exporter\n</code></pre></li> </ul>"},{"location":"zh/installation/community-operator/#operator-huboperator","title":"\u4eceOperator Hub\u5b89\u88c5operator","text":"<ol> <li> <p>\u9009\u4e2dOperators &gt; OperatorHub. \u641c\u7d22 <code>Kepler</code>. \u70b9\u51fb <code>Install</code> </p> </li> <li> <p>\u5141\u8bb8\u5b89\u88c5 </p> </li> <li> <p>\u521b\u5efaKepler\u7684Custom Resource </p> <p>\u6ce8\u610f\uff1a\u5f53\u524d\u7684OCP\u63a7\u5236\u53f0\u53ef\u80fd\u4f1a\u663e\u793a\u4e00\u4e2aJavaScript\u9519\u8bef\uff08\u9884\u8ba1\u5c06\u57284.13.5\u4e2d\u4fee\u590d\uff09\uff0c\u4f46\u5b83\u4e0d\u4f1a\u5f71\u54cd\u5176\u4f59\u6b65\u9aa4\u3002\u4fee\u590d\u7a0b\u5e8f\u76ee\u524d\u53ef\u57284.13.0-0.nightly-2023-07-08-165124\u7248\u672c\u7684OCP\u63a7\u5236\u53f0\u4e0a\u83b7\u5f97\u3002</p> </li> </ol>"},{"location":"zh/installation/community-operator/#grafana-operator","title":"\u5b89\u88c5Grafana operator","text":""},{"location":"zh/installation/community-operator/#grafana-operator_1","title":"\u90e8\u7f72Grafana Operator","text":"<p>\u5f53\u524dAPI Bearer\u4ee4\u724c\u9700\u8981\u5728<code>GrafanaDataSource</code>\u6e05\u5355\u4e2d\u66f4\u65b0\uff0c\u4ee5\u4fbf<code>Grafana DataSource</code>\u53ef\u4ee5\u5411Prometheus\u8fdb\u884c\u8eab\u4efd\u9a8c\u8bc1\u3002\u4ee5\u4e0b\u547d\u4ee4\u5c06\u66f4\u65b0\u6e05\u5355\u5e76\u5728\u547d\u540d\u7a7a\u95f4<code>kepler-operator-system</code>\u4e2d\u90e8\u7f72Grafana Operator</p> <pre><code>BEARER_TOKEN=$(oc whoami --show-token)\nhack/dashboard/openshift/deploy-grafana.sh\n</code></pre> <p>\u6ce8\u610f\uff1a\u811a\u672c\u8981\u6c42\u60a8\u4f4d\u4e8e\u9876\u7ea7\u76ee\u5f55\u4e2d\uff0c\u56e0\u6b64\u8bf7\u786e\u4fdd\u60a8\u4f4d\u4e8e<code>kepler-operator</code>\u6839\u76ee\u5f55\u4e2d\u3002\u4f7f\u7528\u547d\u4ee4<code>cd $(git rev-parse --show-toplevel)</code></p>"},{"location":"zh/installation/community-operator/#garafana-console","title":"\u8bbf\u95eeGarafana Console","text":"<p>\u914d\u7f6eNetworking &gt; Routes.  </p>"},{"location":"zh/installation/community-operator/#grafana-dashboard","title":"Grafana Dashboard","text":"<p>\u4f7f\u7528\u5bc6\u94a5<code>kepler:kepler</code>\u767b\u9646Grafana Dashboard. </p>"},{"location":"zh/installation/community-operator/#_2","title":"\u6545\u969c\u6392\u9664","text":"<p>\u6ce8\u610f\uff1a\u5982\u679c\u6570\u636e\u6e90\u51fa\u73b0\u95ee\u9898\uff0c\u8bf7\u68c0\u67e5API\u4ee4\u724c\u662f\u5426\u5df2\u6b63\u786e\u66f4\u65b0</p> <p></p>"},{"location":"zh/installation/kepler-helm/","title":"\u4f7f\u7528 Helm Chart \u90e8\u7f72","text":"<p>Kepler Helm Chart \u53ef\u5728 GitHub \u548c ArtifactHub \u4e0a\u627e\u5230\u3002</p>"},{"location":"zh/installation/kepler-helm/#helm","title":"\u5b89\u88c5 Helm","text":"<p>\u5fc5\u987b\u5b89\u88c5 Helm \u624d\u80fd\u4f7f\u7528\u56fe\u8868\u3002 \u8bf7\u53c2\u9605 Helm \u7684\u6587\u6863\u4ee5\u5f00\u59cb\u4f7f\u7528\u3002</p>"},{"location":"zh/installation/kepler-helm/#prometheus","title":"Prometheus \u8bbe\u7f6e","text":"<p>Kepler \u5bfc\u51fa\u5668\u9700\u8981\u5df2\u5b89\u88c5 Prometheus\u8282\u70b9\u5bfc\u51fa\u5668 \u3002\u6211\u4eec\u63a8\u8350\u4f7f\u7528 Kube Prometheus Stack Helm \u56fe\u8868\uff0c\u5176\u4e2d\u5305\u62ec Node Exporter\uff0cGrafana \u548c\u5176\u4ed6\u6709\u7528\u7684\u4e1c\u897f\uff0c\u4ee5\u4fbf\u8f7b\u677e\u64cd\u4f5c\u5229\u7528 Prometheus \u8fdb\u884c\u7aef\u5230\u7aef\u7684 Kubernetes \u96c6\u7fa4\u76d1\u63a7\uff0c\u4f7f\u7528 Prometheus \u8fd0\u8425\u5546\u3002</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\n\nhelm install prometheus prometheus-community/kube-prometheus-stack \\\\\n    --namespace monitoring \\\\\n    --create-namespace \\\\\n    --wait\n</code></pre>"},{"location":"zh/installation/kepler-helm/#kepler-helm","title":"\u6dfb\u52a0 Kepler Helm \u4ed3\u5e93","text":"<pre><code>helm repo add kepler https://sustainable-computing-io.github.io/kepler-helm-chart\nhelm repo update\n</code></pre> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u547d\u4ee4\u67e5\u770b\u6700\u65b0\u7248\u672c\uff1a</p> <pre><code>helm search repo kepler\n</code></pre> <p>\u5982\u679c\u60a8\u60f3\u5728\u90e8\u7f72\u524d\u6d4b\u8bd5\u5e76\u67e5\u770b\u6e05\u5355\u6587\u4ef6\uff0c\u60a8\u53ef\u4ee5\u8fd0\u884c\uff1a</p> <pre><code>helm install kepler kepler/kepler --namespace kepler --create-namespace --dry-run --devel\n</code></pre>"},{"location":"zh/installation/kepler-helm/#kepler","title":"\u5b89\u88c5 Kepler","text":"<p>\u4e3a\u4e86\u8ba9 Prometheus \u80fd\u591f\u53d1\u73b0 Kepler \u5bfc\u51fa\u7684\u6307\u6807\uff0c\u9700\u8981\u542f\u7528 <code>serviceMonitor</code> \u5e76\u4e0e\u60a8\u7684 Prometheus \u5b89\u88c5\u7684\u53d1\u5e03\u540d\u79f0\u6807\u8bb0\u3002\u5728\u6211\u4eec\u7684\u5b89\u88c5\u4e2d\uff0c\u6211\u4eec\u5c06 kube-prometheus-stack \u5b89\u88c5\u547d\u540d\u4e3a <code>prometheus</code>\uff1a</p> <pre><code>helm install kepler kepler/kepler \\\\\n    --namespace kepler \\\\\n    --create-namespace \\\\\n    --set serviceMonitor.enabled=true \\\\\n    --set serviceMonitor.labels.release=prometheus \\\\\n</code></pre> <p>\u6216\u8005\uff0c\u60a8\u4e5f\u53ef\u4ee5\u8986\u76d6 values.yaml \u6587\u4ef6\u6765\u8bbe\u7f6e\u8fd9\u4e9b\u548c\u4ee5\u4e0b\u503c:</p> <pre><code>helm install kepler kepler/kepler --values values.yaml --namespace kepler --create-namespace\n</code></pre> <p>\u4ee5\u4e0b\u8868\u683c\u5217\u51fa\u4e86\u6b64\u56fe\u8868\u7684\u53ef\u914d\u7f6e\u53c2\u6570\u53ca\u5176\u9ed8\u8ba4\u503c\u3002</p> \u53c2\u6570 \u63cf\u8ff0 \u9ed8\u8ba4\u503c global.namespace Kepler\u7684Kubernetes\u547d\u540d\u7a7a\u95f4 kepler image.repository Kepler Image\u7684\u5b58\u50a8\u5e93 quay.io/sustainable\\_computing\\_io/kepler image.pullPolicy Kepler\u7684\u62c9\u53d6\u7b56\u7565 \u603b\u662f image.tag Kepler\u56fe\u50cf\u7684\u56fe\u50cf\u6807\u7b7e \u6700\u65b0 serviceAccount.name Kepler\u7684\u670d\u52a1\u5e10\u6237\u540d\u79f0 kepler-sa service.type Kepler\u670d\u52a1\u7c7b\u578b ClusterIP service.port Kepler\u66b4\u9732\u7684\u670d\u52a1\u7aef\u53e3 9102"},{"location":"zh/installation/kepler-helm/#_1","title":"\u5b89\u88c5\u540e","text":"<p>\u5b89\u88c5\u540e\uff0c\u60a8\u53ef\u4ee5\u7b49\u5f85 Kepler \u51c6\u5907\u5c31\u7eea\uff1a</p> <pre><code>KPLR_POD=$(\n    kubectl get pod \\\\\n        -l app.kubernetes.io/name=kepler \\\\\n        -o jsonpath=\\\"{.items[0].metadata.name}\\\" \\\\\n        -n kepler\n)\nkubectl wait --for=condition=Ready pod $KPLR_POD --timeout=-1s -n kepler\n</code></pre> <p>\u5e76\u5c06 Kepler\u4eea\u8868\u76d8 \u6dfb\u52a0\u5230 Grafana\uff1a</p> <pre><code>GF_POD=$(\n    kubectl get pod \\\\\n        -n monitoring \\\\\n        -l app.kubernetes.io/name=grafana \\\\\n        -o jsonpath=\\\"{.items[0].metadata.name}\\\"\n)\nkubectl cp kepler_dashboard.json monitoring/$GF_POD:/tmp/dashboards/kepler_dashboard.json\n</code></pre>"},{"location":"zh/installation/kepler-helm/#kepler_1","title":"\u5378\u8f7d Kepler","text":"<p>\u8981\u5378\u8f7d\u6b64\u56fe\u8868\uff0c\u8bf7\u4f7f\u7528\u4ee5\u4e0b\u6b65\u9aa4</p> <pre><code>helm delete kepler --namespace kepler\n</code></pre>"},{"location":"zh/installation/kepler-operator/","title":"\u901a\u8fc7Kepler Operator\u5728Kind\u4e0a\u5b89\u88c5","text":""},{"location":"zh/installation/kepler-operator/#_1","title":"\u9700\u6c42","text":"<p>\u5728\u5f00\u59cb\u524d\u8bf7\u786e\u8ba4\u60a8\u5df2\u7ecf\u5b89\u88c5\u4e86:</p> <ul> <li><code>kubectl</code></li> <li>\u4e0b\u8f7d\u4e86<code>kepler-operator</code>repository</li> <li>\u76ee\u6807k8s\u96c6\u7fa4\u3002\u60a8\u53ef\u4ee5\u4f7f\u7528Kind\u6765\u7b80\u5355\u6784\u5efa\u4e00\u4e2a\u672c\u5730k8s\u96c6\u7fa4\u6765\u4f53\u9a8c\u672c\u6559\u7a0b\u3002\u542f\u52a8\u4e00\u4e2a\u672c\u5730kind\u96c6\u7fa4, \u6216\u76f4\u63a5\u5728\u60a8\u8fdc\u7aef\u7684k8s\u96c6\u7fa4\u6267\u884c\u3002\u6ce8\u610f\u60a8\u7684controller\u5c06\u4f1a\u81ea\u52a8\u4f7f\u7528\u5f53\u524d\u7684kubeconfig\u914d\u7f6e\u6587\u4ef6\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7<code>kubectl cluster-info</code>\u6765\u67e5\u770b\u3002</li> <li>\u6709<code>kubeadmin</code> \u6216\u8005 <code>cluster-admin</code> \u6743\u9650\u7684\u7528\u6237\u3002</li> </ul>"},{"location":"zh/installation/kepler-operator/#kind","title":"\u542f\u52a8\u4e00\u4e2a\u672c\u5730kind\u96c6\u7fa4","text":"<pre><code>cd kepler-operator\nmake cluster-up CLUSTER_PROVIDER='kind' CI_DEPLOY=true GRAFANA_ENABLE=true\n\nkubectl get pods -n monitoring\n\ngrafana-b88df6989-km7c6                1/1     Running   0          48m\nprometheus-k8s-0                       2/2     Running   0          46m\nprometheus-operator-6bd88c8bdf-9f69h   2/2     Running   0          48m\n</code></pre>"},{"location":"zh/installation/kepler-operator/#kepler-operator","title":"\u542f\u52a8kepler-operator","text":"<ul> <li> <p>\u60a8\u53ef\u4ee5\u901a\u8fc7quay.io\u4e0a\u7684image\u6765\u90e8\u7f72kepler-operator.</p> <pre><code>make deploy IMG=quay.io/sustainable_computing_io/kepler-operator:latest\nkubectl config set-context --current --namespace=monitoring\nkubectl apply -k config/samples/\n</code></pre> </li> <li> <p>\u901a\u8fc7<code>kubectl get pods -n monitoring</code>\u547d\u4ee4\u6765\u9a8c\u8bc1<code>kepler-exporter</code>pod\u7684\u90e8\u7f72\u60c5\u51b5\u3002</p> </li> </ul>"},{"location":"zh/installation/kepler-operator/#grafana-dashboard","title":"\u8bbe\u7f6eGrafana Dashboard","text":"<p>\u4f7f\u7528<code>GRAFANA_ENABLE=true</code> \u6765\u914d\u7f6e<code>kube-prometheus</code>\u5728\u547d\u540d\u7a7a\u95f4<code>monitoring</code>\u4e0a\u7684\u90e8\u7f72. \u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u6765\u8bbf\u95ee\u4f4d\u4e8e3000\u7aef\u53e3\u7684grafana\u754c\u9762\u3002</p> <pre><code>kubectl port-forward svc/grafana 3000:3000 -n monitoring\n</code></pre> <p>\u5e76\u901a\u8fc7\u4ee5\u4e0b\u57df\u540d\u8bbf\u95eehttp://localhost:3000</p>"},{"location":"zh/installation/kepler-operator/#service-monitor","title":"Service Monitor","text":"<p>\u8ba9<code>kube-prometheus</code> \u4f7f\u7528 <code>kepler-exporter</code> \u670d\u52a1\u7aef\u53e3\u8fdb\u884c\u76d1\u63a7\uff0c\u60a8\u9700\u8981\u914d\u7f6eservice monitor.</p> <p>Note</p> <p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b<code>kube-prometheus</code> \u4e0d\u4f1a\u6355\u6349<code>monitoring</code>\u547d\u540d\u7a7a\u95f4\u4e4b\u5916\u7684\u670d\u52a1. \u5982\u679c\u60a8\u7684kepler\u90e8\u7f72\u5728<code>monitoring</code>\u7a7a\u95f4\u4e4b\u5916\u76d1\u63a7\u6240\u6709\u7684\u547d\u540d\u7a7a\u95f4.</p> <pre><code>kubectl apply -n monitoring -f - &lt;&lt; EOF\napiVersion: monitoring.coreos.com/v1\nkind: ServiceMonitor\nmetadata:\n  labels:\n    app.kubernetes.io/component: exporter\n    app.kubernetes.io/name: kepler-exporter\n    sustainable-computing.io/app: kepler\n  name: monitor-kepler-exporter\nspec:\n  endpoints:\n  - interval: 3s\n    port: http\n    relabelings:\n    - action: replace\n      regex: (.*)\n      replacement: $1\n      sourceLabels:\n      - __meta_kubernetes_pod_node_name\n      targetLabel: instance\n    scheme: http\n  jobLabel: app.kubernetes.io/name\n  namespaceSelector:\n    matchNames:\n    any: true\n  selector:\n    matchLabels:\n      app.kubernetes.io/component: exporter\n      app.kubernetes.io/name: kepler-exporter\nEOF\n</code></pre>"},{"location":"zh/installation/kepler-operator/#grafana-dashboard_1","title":"Grafana Dashboard","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u6b65\u9aa4\u914d\u7f6eGrafana:</p> <ul> <li>\u767b\u9646localhost:3000\u9ed8\u8ba4\u7528\u6237\u540d/\u5bc6\u7801\u4e3a<code>admin:admin</code></li> <li>\u5012\u5165\u9ed8\u8ba4dashboard</li> </ul> <p></p>"},{"location":"zh/installation/kepler-operator/#operator","title":"\u5378\u8f7doperator","text":"<p>\u901a\u8fc7\u4ee5\u4e0b\u547d\u4ee4\u5378\u8f7d: <pre><code>make undeploy\n</code></pre></p> <p>\u53c2\u8003\u8fd9\u91cc \u6765\u8ba9kepler operator\u8fd0\u884c\u5728kind\u96c6\u7fa4\u4e0a\u3002</p>"},{"location":"zh/installation/kepler-operator/#_2","title":"\u9519\u8bef\u6392\u67e5","text":""},{"location":"zh/installation/kepler-operator/#_3","title":"\u76d1\u63a7\u6240\u6709\u7684\u547d\u540d\u7a7a\u95f4","text":"<p>kube-prometheus\u9ed8\u8ba4\u4e0d\u4f1a\u76d1\u63a7\u6240\u6709\u7684\u547d\u540d\u7a7a\u95f4\uff0c\u8fd9\u662f\u7531\u4e8eRBAC\u63a7\u5236\u7684\u3002 \u4ee5\u4e0bclusterrole <code>prometheus-k8s</code>\u7684\u914d\u7f6e\u8bb2\u5141\u8bb8kube-prometheus\u76d1\u63a7\u6240\u6709\u547d\u540d\u7a7a\u95f4\u3002</p> <pre><code>oc describe clusterrole prometheus-k8s\nName:         prometheus-k8s\nLabels:       app.kubernetes.io/component=prometheus\n              app.kubernetes.io/instance=k8s\n              app.kubernetes.io/name=prometheus\n              app.kubernetes.io/part-of=kube-prometheus\n              app.kubernetes.io/version=2.45.0\nAnnotations:  &lt;none&gt;\nPolicyRule:\n  Resources                    Non-Resource URLs  Resource Names  Verbs\n  ---------                    -----------------  --------------  -----\n  endpoints                    []                 []              [get list watch]\n  pods                         []                 []              [get list watch]\n  services                     []                 []              [get list watch]\n  ingresses.networking.k8s.io  []                 []              [get list watch]\n                               [/metrics]         []              [get]\n  nodes/metrics                []                 []              [get]\n</code></pre> <ul> <li> <p>\u5728\u521b\u5efa\u542f\u52a8\u4e00\u4e2a\u672c\u5730kind\u96c6\u7fa4\u5b9a\u5236prometheus\uff0c\u8bf7\u53c2\u8003 kube-prometheus\u6587\u6863Customizing Kube-Prometheus</p> </li> <li> <p>\u8bf7\u786e\u5b9a\u60a8\u5e94\u7528\u4e86this jsonnet\u4fdd\u8bc1prometheus\u76d1\u63a7\u6240\u6709\u547d\u540d\u7a7a\u95f4\u3002</p> </li> </ul>"},{"location":"zh/project/resources/","title":"Resources","text":""},{"location":"zh/project/resources/#_1","title":"\u6f14\u8bb2\u548c\u6f14\u793a","text":"<p>\u6211\u4eec\u975e\u5e38\u611f\u8c22\u6765\u81ea\u793e\u533a\u7684\u5173\u4e8e\u5f00\u666e\u52d2\u7684\u8bb2\u5ea7\u548c\u6f14\u793a\u3002\u5982\u679c\u60a8\u505a\u4e86\u6f14\u793a\u6216\u5f15\u7528\u5f00\u666e\u52d2\u7684\u6f14\u793a\uff0c\u8bf7\u6253\u5f00PR\u5c06\u5176\u6dfb\u52a0\u5230\u6b64\u9875\u9762\uff01</p>"},{"location":"zh/project/resources/#_2","title":"\u6f14\u793a","text":"<p>\u4e0b\u9762\u7684\u5217\u8868\u5305\u542b\u6f14\u793akepler\u3001\u5176\u529f\u80fd\u4ee5\u53ca\u5982\u4f55\u6536\u96c6\u5404\u79cdKubernetes\u8d44\u6e90\u7684\u80fd\u91cf\u6307\u6807\u7684\u8bb2\u5ea7\u3002</p> <ul> <li> <p>\"Sustainability the Container Native Way\", Huamin Chen (Red Hat) &amp; Chen Wang (IBM), Open Source Summit NA 2022 [slides]</p> </li> <li> <p>\"Sustainability Research the Cloud Native Way\", Chen Wang (IBM) &amp; Huamin Chen (Red Hat), KubeCon NA 2022 [slides]</p> </li> <li> <p>\"Sustainability in Computing: Energy Efficient Placements of Edge Workloads\", Parul Singh &amp; Kaiyi Liu (Red Hat), Kubernetes Edge Day NA 2022 [slides]</p> </li> <li> <p>\"Green(ing) CI/CD: A Sustainability Journey with GitOps\", Niki Manoledaki (Weaveworks), GitOpsCon NA 2022 [slides]</p> </li> </ul>"},{"location":"zh/project/resources/#kepler","title":"Kepler\u53c2\u8003\u8d44\u6599","text":"<p>\u4e0b\u9762\u7684\u5217\u8868\u5305\u542b\u4e86\u5728\u8ba8\u8bba\u80fd\u6e90\u6548\u7387\u548c\u4e91\u539f\u751f\u73af\u5883\u53ef\u6301\u7eed\u6027\u65f6\u63d0\u5230\u5f00\u666e\u52d2\u7684\u8c08\u8bdd\u3002</p> <ul> <li> <p>\"Panel Discussion: Moving Towards Environmentally Sustainable Operations with Cloud Native Tools\", Niki Manoledaki (Weaveworks), Chris Lavery (Weaveworks), Marlow Weston (Intel), William Caban (Red Hat) [recording]</p> </li> <li> <p>\"How to Get Involved in CNCF Environmental Sustainability TAG\", Marlow Weston (Intel) &amp; Huamin Chen (Red Hat), KubeCon NA 22 [recording]</p> </li> <li> <p>\"Smart Green Computing Cloud Native Operations\", William Caban &amp; Federico Rossi (Red Hat), KubeCon NA 2022 [recording]</p> </li> <li> <p>\u901a\u8fc7Istio\u3001Kepler\u548c\u667a\u80fd\u8c03\u5ea6\u5b9e\u73b0\u4f18\u5316\u7684\u5fae\u670d\u52a1\u6027\u80fd\u548c\u53ef\u6301\u7eed\u6027 | Towards Optimized Microservices Performance &amp; Sustainability via Istio, Kepler and Smart Scheduling - Peng Hui Jiang &amp; Kevin Su, IBM &amp; Yingchun Guo, Intel, KubeCon China 2023 link</p> </li> <li> <p>\u5728\u7279\u5b9a\u5e73\u53f0\u4e0a\uff0c\u5f00\u666e\u52d2\u51c6\u786e\u5417\uff1f | Is Kepler Accurate on Specific Platforms? - Jie Ren &amp; Ken Lu, Intel, KubeCon China 2023 link</p> </li> <li> <p>kepler 101, Sam Yuan(IBM), KubeCon China 2023 kiosk</p> </li> <li> <p>kepler \u90e8\u7f72, Sam Yuan(IBM), KubeCon China 2023 kiosk</p> </li> <li> <p>kepler \u6784\u5efa, Sam Yuan(IBM), KubeCon China 2023 kiosk</p> </li> <li> <p>kepler \u80fd\u8017\u6a21\u578b\u8bad\u7ec3, Sam Yuan(IBM), KubeCon China 2023 kiosk, recording in Chinese</p> </li> </ul>"}]}